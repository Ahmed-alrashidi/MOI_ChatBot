# =========================================================================
# File Name: ui/app.py
# Project: Absher Smart Assistant (MOI ChatBot)
# Purpose: Gradio User Interface (Enhanced UX & Multilingual Support)
# Features:
# - Clean Text Response: Optimized for high-speed streaming without artifacts.
# - On-Demand Audio: Manual trigger for Neural TTS to save server compute.
# - Dynamic RTL/LTR Logic: Switches layout based on language selection.
# - ASR Integration: Seamless voice-to-text transcription using Whisper.
# =========================================================================

import gradio as gr
import time
from typing import List, Optional, Any

from config import Config
from core.model_loader import ModelManager
from ui.theme import MOI_CSS, HEADER_HTML, MOI_THEME, THEME_JS, TOGGLE_JS
from utils.logger import setup_logger
from utils.tts import generate_speech

# Initialize a dedicated UI logger for tracking user interactions
logger = setup_logger("UI_App")

def create_app(rag_system: Any) -> gr.Blocks:
    """
    Constructs the central Gradio UI blocks. This function defines the 
    visual layout, state management, and event-driven logic of the application.

    Args:
        rag_system: An initialized instance of the RAGPipeline class.

    Returns:
        gr.Blocks: The compiled Gradio interface object.
    """

    # --- 1. INTERACTION LOGIC ---
    
    def chat_pipeline(
        user_text: str,
        history: List[List[str]],
        audio_path: Optional[str],
        selected_lang: str,
    ):
        """
        The primary message handler. Orchestrates the flow from raw input 
        (text or voice) to the RAG engine and back to the UI.

        Execution Flow:
        1. Transcribe voice input (if provided) via Whisper.
        2. Immediate UI update to show user message.
        3. Trigger RAG pipeline for reasoning and context retrieval.
        4. Stream the text response back to the chatbot window.
        """
        # Guard: Ensure there is some form of input
        if not user_text and not audio_path:
            yield history, None, "", None
            return

        # Initialize history if the session is fresh
        if history is None:
            history = []

        # A. Voice Processing: Convert Speech to Text (ASR)
        if audio_path:
            logger.info("ğŸ¤ Processing Audio Input...")
            try:
                asr_pipe = ModelManager.get_asr_pipeline()
                if asr_pipe:
                    # Transcribe using OpenAI Whisper Large-v3
                    out = asr_pipe(audio_path)
                    transcribed = out.get("text", "").strip()
                    if transcribed:
                        user_text = transcribed
                else:
                    logger.warning("âš ï¸ ASR Model not loaded in ModelManager.")
            except Exception as e:
                logger.error(f"âŒ ASR Error: {e}")

        # B. UX Update: Show the user's message immediately for responsiveness
        history = history + [[user_text, None]]
        yield history, None, "", None

        # C. RAG Generation Core
        start_time = time.time()
        try:
            # Prepare conversation history (stripping current turn)
            context_history = [tuple(h) for h in history[:-1]]

            # Execute reasoning via ALLaM-7B
            response_text = rag_system.run(user_text, context_history)

            latency = time.time() - start_time
            logger.info(f"âœ… Response generated in {latency:.2f}s")

        except Exception as e:
            logger.error(f"âŒ RAG Error during generation: {e}")
            response_text = (
                "Ø¹Ø°Ø±Ø§Ù‹ØŒ ÙˆØ§Ø¬Ù‡Øª Ù…Ø´ÙƒÙ„Ø© ØªÙ‚Ù†ÙŠØ©. ÙŠØ±Ø¬Ù‰ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ù…Ø±Ø© Ø£Ø®Ø±Ù‰.\n"
                "Sorry, a technical error occurred."
            )

        # D. Delivery: Update the chatbot with the final AI response
        history[-1][1] = response_text
        # Pass None to tts_player to keep it silent until manual request
        yield history, None, "", None 

    def speak_last_response(history):
        """
        On-Demand TTS Handler. Generates high-fidelity neural audio only 
        when the user explicitly clicks the 'Listen' button. 
        This prevents unnecessary API/GPU load during text-only interactions.
        """
        if not history:
            return None
            
        # Extract the last message generated by the assistant
        last_response = history[-1][1]
        
        if last_response:
            logger.info("ğŸ”Š Manual Neural TTS Triggered")
            try:
                # Generate MP3 via MS Edge Neural Engine
                return generate_speech(last_response)
            except Exception as e:
                logger.error(f"âŒ Manual TTS Generation Error: {e}")
        return None

    # --- 2. UX HELPERS ---

    def update_layout(lang_choice: str):
        """
        UI Adaptation Logic: Toggles Right-to-Left (RTL) or Left-to-Right (LTR) 
        rendering based on the selected language to ensure correct typography.
        """
        ltr_langs = ["English", "French", "German", "Chinese", "Russian", "Spanish"]

        # If a Western/Asian language is selected, disable RTL
        if lang_choice and any(x in lang_choice for x in ltr_langs):
            return (
                gr.Chatbot.update(rtl=False),
                gr.Textbox.update(
                    rtl=False,
                    placeholder="Type your query here...",
                    label="Your Question",
                ),
            )
        # Default to RTL for Arabic/Urdu or Auto-Detect
        else:
            return (
                gr.Chatbot.update(rtl=True),
                gr.Textbox.update(
                    rtl=True,
                    placeholder="Ø§ÙƒØªØ¨ Ø§Ø³ØªÙØ³Ø§Ø±Ùƒ Ù‡Ù†Ø§...",
                    label="Ø§Ù„Ø§Ø³ØªÙØ³Ø§Ø± Ø§Ù„Ù†ØµÙŠ",
                ),
            )

    def clear_context():
        """Resets the chat interface and clears temporary memory."""
        return [], None, "", None

    # --- 3. UI CONSTRUCTION (Gradio Blocks) ---

    with gr.Blocks(
        theme=MOI_THEME,
        css=MOI_CSS,
        title="Absher AI Assistant",
    ) as demo:

        # Header Branding Section
        with gr.Row(elem_id="header-container"):
            with gr.Column(scale=9):
                gr.HTML(HEADER_HTML) # Inject official branding and logos
            with gr.Column(scale=1, min_width=50):
                # Dark Mode Toggle Switch
                theme_btn = gr.Button(
                    "ğŸŒ—",
                    variant="secondary",
                    elem_classes=["badge"],
                    size="sm",
                )

        # Main Layout: Chat vs. Settings
        with gr.Row():
            # Left Column: The Conversation Hub
            with gr.Column(scale=2):
                chatbot = gr.Chatbot(
                    label="Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© | Conversation",
                    show_label=False,
                    avatar_images=(None, "ui/assets/moi_logo.png"),
                    height=600,
                    rtl=True,
                    elem_classes=["chatbot-container"],
                    show_copy_button=True,
                )

                # Invisible/Persistent Audio Player for Voice Output
                tts_player = gr.Audio(
                    label="ğŸ”Š Ø§Ù„Ø§Ø³ØªÙ…Ø§Ø¹ Ù„Ù„Ø±Ø¯ (Voice Response)",
                    autoplay=True, # Plays automatically only when source is updated
                    visible=True,
                    elem_id="tts-player",
                    interactive=False,
                )

            # Right Column: Configuration & Controls
            with gr.Column(scale=1, elem_classes=["controls-panel"]):
                gr.Markdown("### âš™ï¸ Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª | Settings")

                lang_dropdown = gr.Dropdown(
                    choices=[
                        "Auto-Detect (ØªÙ„Ù‚Ø§Ø¦ÙŠ)",
                        "Arabic (Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©)",
                        "English",
                        "Urdu (Ø£Ø±Ø¯Ùˆ)",
                        "French",
                        "Spanish",
                        "Chinese",
                        "Russian",
                        "German",
                    ],
                    value="Auto-Detect (ØªÙ„Ù‚Ø§Ø¦ÙŠ)",
                    label="Ø§Ù„Ù„ØºØ© | Language",
                    interactive=True,
                )

                gr.Markdown("---")
                gr.Markdown("### ğŸ—¨ï¸ Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª | Input")

                msg_input = gr.Textbox(
                    placeholder="Ø§ÙƒØªØ¨ Ø§Ø³ØªÙØ³Ø§Ø±Ùƒ Ù‡Ù†Ø§...",
                    label="Ø§Ù„Ø§Ø³ØªÙØ³Ø§Ø± Ø§Ù„Ù†ØµÙŠ",
                    lines=3,
                    rtl=True,
                    elem_id="msg-input",
                )

                # Microphone for Voice Interaction
                audio_input = gr.Audio(
                    source="microphone",
                    type="filepath",
                    label="ğŸ¤ ØªØ³Ø¬ÙŠÙ„ ØµÙˆØªÙŠ (Voice Input)",
                    elem_id="audio-input",
                )

                with gr.Row():
                    clear_btn = gr.Button("ğŸ—‘ï¸ Ù…Ø³Ø­", variant="secondary")
                    # Button to manually trigger neural voice synthesis
                    tts_btn = gr.Button("ğŸ”Š Ù†Ø·Ù‚ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©", variant="secondary")
                    submit_btn = gr.Button("ğŸš€ Ø¥Ø±Ø³Ø§Ù„", variant="primary")

        # --- 4. EVENT WIRING ---

        # Trigger Layout Change on Language selection
        lang_dropdown.change(
            fn=update_layout,
            inputs=[lang_dropdown],
            outputs=[chatbot, msg_input],
        )

        # Toggle Theme via JS injection
        theme_btn.click(None, None, None, _js=TOGGLE_JS)

        # Message Submission via Button or Keyboard
        submit_btn.click(
            chat_pipeline,
            inputs=[msg_input, chatbot, audio_input, lang_dropdown],
            outputs=[chatbot, tts_player, msg_input, audio_input],
        )

        msg_input.submit(
            chat_pipeline,
            inputs=[msg_input, chatbot, audio_input, lang_dropdown],
            outputs=[chatbot, tts_player, msg_input, audio_input],
        )
        
        # Trigger TTS specifically for the assistant's last response
        tts_btn.click(
            fn=speak_last_response,
            inputs=[chatbot],
            outputs=[tts_player],
        )

        # Global Clear functionality
        clear_btn.click(
            clear_context,
            outputs=[chatbot, tts_player, msg_input, audio_input],
        )

        # Persistent Theme Loading logic
        try:
            demo.load(None, None, None, _js=THEME_JS)
        except Exception:
            pass

    return demo