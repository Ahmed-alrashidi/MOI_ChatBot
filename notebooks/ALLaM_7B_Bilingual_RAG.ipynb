{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a1f3e8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Initializing Enhanced CELL 0 Environment Setupâ€¦\n",
      "\n",
      "--- 1. Checking GPU Hardware ---\n",
      "âœ… GPU detected: NVIDIA A100-SXM4-80GB\n",
      "âœ… CUDA version: 12.8\n",
      "\n",
      "--- 2. Validating Python Dependencies ---\n",
      "âœ… All required packages already installed.\n",
      "\n",
      "--- 3. Checking system dependencies (ffmpeg) ---\n",
      "âœ… ffmpeg detected.\n",
      "\n",
      "--- 4. Creating Project Directory Structure ---\n",
      "âœ… Directory structure initialized under: chatbot_project\n",
      "\n",
      "--- 5. Initializing Logger ---\n",
      "ğŸ“ Logging to: chatbot_project/4_outputs/logs/app.log\n",
      "\n",
      "--- 6. Validating Master + Chunk CSVs ---\n",
      "âœ… Master CSVs folder detected: /ibex/user/rashidah/projects/MOI_ChatBot/chatbot_project/1_data/Data_Master\n",
      "âœ… Chunk CSVs folder detected: /ibex/user/rashidah/projects/MOI_ChatBot/chatbot_project/1_data/Data_chunks\n",
      "ğŸ“¦ Master files found: 10\n",
      "ğŸ“¦ Chunk files found:  10\n",
      "\n",
      "ğŸ” Validating CSV schemasâ€¦\n",
      "   âœ… OK â†’ ahwal_master.csv\n",
      "   âœ… OK â†’ amn_master.csv\n",
      "   âœ… OK â†’ hajj_master.csv\n",
      "   âœ… OK â†’ ahwal_rag_chunks.csv\n",
      "   âœ… OK â†’ amn_rag_chunks.csv\n",
      "   âœ… OK â†’ hajj_rag_chunks.csv\n",
      "\n",
      "ğŸ‰ CELL 0 COMPLETE â€” Environment & CSV Knowledge Base Loaded Successfully.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# @title âš™ï¸ğŸ§± CELL 0 â€” Environment & CSV Knowledge Base Setup (Production-Ready)\n",
    "# English-only comments as requested.\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import torch\n",
    "import warnings\n",
    "import importlib.util\n",
    "import subprocess\n",
    "import shutil\n",
    "from typing import List\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "\n",
    "print(\"ğŸš€ Initializing Enhanced CELL 0 Environment Setupâ€¦\")\n",
    "\n",
    "# ================================================================================\n",
    "# 1) GPU Hardware Validation\n",
    "# ================================================================================\n",
    "print(\"\\n--- 1. Checking GPU Hardware ---\")\n",
    "if not torch.cuda.is_available():\n",
    "    warnings.warn(\"CUDA not detected â€” GPU acceleration unavailable.\")\n",
    "    print(\"ğŸ”¥ WARNING: No GPU detected. Performance will be slow.\")\n",
    "else:\n",
    "    print(f\"âœ… GPU detected: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"âœ… CUDA version: {torch.version.cuda}\")\n",
    "\n",
    "# ================================================================================\n",
    "# 2) Python Dependency Validation\n",
    "# ================================================================================\n",
    "print(\"\\n--- 2. Validating Python Dependencies ---\")\n",
    "\n",
    "required_packages = {\n",
    "    \"transformers\": \"transformers\",\n",
    "    \"accelerate\": \"accelerate\",\n",
    "    \"sentencepiece\": \"sentencepiece\",\n",
    "\n",
    "    \"langchain\": \"langchain\",\n",
    "    \"langchain-community\": \"langchain_community\",\n",
    "    \"langchain-huggingface\": \"langchain_huggingface\",\n",
    "    \"chromadb\": \"chromadb\",\n",
    "\n",
    "    \"sentence-transformers\": \"sentence_transformers\",\n",
    "    \"tiktoken\": \"tiktoken\",\n",
    "\n",
    "    \"pandas\": \"pandas\",\n",
    "    \"pyarrow\": \"pyarrow\",\n",
    "    \"tqdm\": \"tqdm\",\n",
    "}\n",
    "\n",
    "missing_pkgs = []\n",
    "for pip_name, import_name in required_packages.items():\n",
    "    if importlib.util.find_spec(import_name) is None:\n",
    "        missing_pkgs.append(pip_name)\n",
    "        print(f\"âš ï¸ Missing: {import_name} â†’ installing ({pip_name})\")\n",
    "\n",
    "if missing_pkgs:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--quiet\", \"--upgrade\"] + missing_pkgs)\n",
    "    print(\"âœ… Installed missing packages.\")\n",
    "else:\n",
    "    print(\"âœ… All required packages already installed.\")\n",
    "\n",
    "# ================================================================================\n",
    "# 3) System Dependencies (ffmpeg for Whisper)\n",
    "# ================================================================================\n",
    "print(\"\\n--- 3. Checking system dependencies (ffmpeg) ---\")\n",
    "if shutil.which(\"ffmpeg\") is None:\n",
    "    print(\"âš ï¸ ffmpeg NOT FOUND â€” audio transcription will fail.\")\n",
    "else:\n",
    "    print(\"âœ… ffmpeg detected.\")\n",
    "\n",
    "# ================================================================================\n",
    "# 4) Project Config (Upgraded)\n",
    "# ================================================================================\n",
    "class Config:\n",
    "    # HF Models\n",
    "    EMBEDDING_MODEL_NAME = \"BAAI/bge-m3\"\n",
    "    LLM_MODEL_NAME = \"ALLaM-AI/ALLaM-7B-Instruct-preview\"\n",
    "    ASR_MODEL_NAME = \"openai/whisper-large-v3\"\n",
    "\n",
    "    # RAG Engine Settings\n",
    "    CHUNK_SIZE = 1500\n",
    "    CHUNK_OVERLAP = 250\n",
    "    MEMORY_WINDOW = 4\n",
    "    SERVICES_PER_DISPLAY_PAGE = 5\n",
    "    ARABIC_NORMALIZATION = True\n",
    "    ENABLE_QUERY_REWRITER = True\n",
    "    ENABLE_ENGLISH_TRANSLATION = True\n",
    "\n",
    "    # Absolute CSV paths\n",
    "    DATA_MASTER_DIR = \"/ibex/user/rashidah/projects/MOI_ChatBot/chatbot_project/1_data/Data_Master\"\n",
    "    DATA_CHUNKS_DIR = \"/ibex/user/rashidah/projects/MOI_ChatBot/chatbot_project/1_data/Data_chunks\"\n",
    "\n",
    "    # Project Folder Structure\n",
    "    PROJECT_DIR = \"chatbot_project\"\n",
    "    DATA_DIR = os.path.join(PROJECT_DIR, \"1_data\")\n",
    "    PROCESSED_DIR = os.path.join(PROJECT_DIR, \"2_processed\")\n",
    "    MODELS_DIR = os.path.join(PROJECT_DIR, \"3_models\")\n",
    "    OUTPUTS_DIR = os.path.join(PROJECT_DIR, \"4_outputs\")\n",
    "    LOGS_DIR = os.path.join(OUTPUTS_DIR, \"logs\")\n",
    "    AUDIO_DIR = os.path.join(OUTPUTS_DIR, \"audio\")\n",
    "    VECTOR_DB_DIR = os.path.join(PROJECT_DIR, \"vector_db\")\n",
    "\n",
    "    # Logging\n",
    "    LOG_FILE = os.path.join(LOGS_DIR, \"app.log\")\n",
    "\n",
    "# ================================================================================\n",
    "# 5) Create Required Directories\n",
    "# ================================================================================\n",
    "print(\"\\n--- 4. Creating Project Directory Structure ---\")\n",
    "\n",
    "for folder in [\n",
    "    Config.PROJECT_DIR,\n",
    "    Config.DATA_DIR,\n",
    "    Config.PROCESSED_DIR,\n",
    "    Config.MODELS_DIR,\n",
    "    Config.OUTPUTS_DIR,\n",
    "    Config.LOGS_DIR,\n",
    "    Config.AUDIO_DIR,\n",
    "    Config.VECTOR_DB_DIR,\n",
    "]:\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "print(f\"âœ… Directory structure initialized under: {Config.PROJECT_DIR}\")\n",
    "\n",
    "# ================================================================================\n",
    "# 6) Logging Initialization\n",
    "# ================================================================================\n",
    "print(\"\\n--- 5. Initializing Logger ---\")\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    filename=Config.LOG_FILE,\n",
    "    filemode=\"w\",\n",
    "    format=\"%(asctime)s - %(levelname)s - [%(filename)s:%(lineno)d] - %(message)s\",\n",
    "    force=True\n",
    ")\n",
    "\n",
    "logging.info(\"Enhanced CELL 0 initialized successfully.\")\n",
    "print(f\"ğŸ“ Logging to: {Config.LOG_FILE}\")\n",
    "\n",
    "# ================================================================================\n",
    "# 7) Validate the CSV Knowledge Base\n",
    "# ================================================================================\n",
    "print(\"\\n--- 6. Validating Master + Chunk CSVs ---\")\n",
    "\n",
    "MASTER_SCHEMA = [\n",
    "    \"service_id\", \"service_title_ar\", \"description_full\",\n",
    "    \"beneficiaries\", \"fees\", \"conditions\", \"access_path\",\n",
    "]\n",
    "\n",
    "CHUNK_SCHEMA = [\n",
    "    \"chunk_id\", \"service_id\", \"chunk_title\",\n",
    "    \"chunk_text\", \"chunk_type\", \"language\", \"meta\",\n",
    "]\n",
    "\n",
    "def ensure_folder(path, name):\n",
    "    if not os.path.isdir(path):\n",
    "        raise FileNotFoundError(f\"âŒ {name} folder not found: {path}\")\n",
    "    print(f\"âœ… {name} folder detected: {path}\")\n",
    "\n",
    "def list_csv(path):\n",
    "    return sorted(glob(os.path.join(path, \"*.csv\")))\n",
    "\n",
    "def validate_schema(csv_file, schema):\n",
    "    df = pd.read_csv(csv_file, nrows=2)\n",
    "    missing = [c for c in schema if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"âŒ Schema mismatch in {csv_file}: missing {missing}\")\n",
    "\n",
    "ensure_folder(Config.DATA_MASTER_DIR, \"Master CSVs\")\n",
    "ensure_folder(Config.DATA_CHUNKS_DIR, \"Chunk CSVs\")\n",
    "\n",
    "master_files = list_csv(Config.DATA_MASTER_DIR)\n",
    "chunks_files = list_csv(Config.DATA_CHUNKS_DIR)\n",
    "\n",
    "print(f\"ğŸ“¦ Master files found: {len(master_files)}\")\n",
    "print(f\"ğŸ“¦ Chunk files found:  {len(chunks_files)}\")\n",
    "\n",
    "print(\"\\nğŸ” Validating CSV schemasâ€¦\")\n",
    "for f in master_files[:3]:\n",
    "    validate_schema(f, MASTER_SCHEMA)\n",
    "    print(f\"   âœ… OK â†’ {os.path.basename(f)}\")\n",
    "\n",
    "for f in chunks_files[:3]:\n",
    "    validate_schema(f, CHUNK_SCHEMA)\n",
    "    print(f\"   âœ… OK â†’ {os.path.basename(f)}\")\n",
    "\n",
    "print(\"\\nğŸ‰ CELL 0 COMPLETE â€” Environment & CSV Knowledge Base Loaded Successfully.\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6eb3aadc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Starting CELL 1 â€” CSV Knowledge Base Processingâ€¦\n",
      "ğŸ“¦ Master files: 10\n",
      "ğŸ“¦ Chunk files : 10\n",
      "ğŸ§¾ Master rows: 64\n",
      "ğŸ§© Chunk rows : 256\n",
      "ğŸ§± Service docs built: 64\n",
      "âœ‚ï¸ Chunk documents prepared: 256\n",
      "ğŸ’¾ Preview saved â†’ chatbot_project/2_processed/service_docs_preview.jsonl\n",
      "ğŸ’¾ Stats saved   â†’ chatbot_project/2_processed/chunk_docs_stats.json\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h2 style='text-align:center;color:green;'>âœ… CELL 1 completed successfully</h2>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ‰ CELL 1 COMPLETE.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# @title âš™ï¸ğŸ§± CELL 1 â€” Data Processing for CSV Knowledge Base (Masters + Chunks)\n",
    "# English-only comments as requested.\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import glob\n",
    "import logging\n",
    "import unicodedata\n",
    "from typing import List, Dict, Any\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "print(\"ğŸ“Š Starting CELL 1 â€” CSV Knowledge Base Processingâ€¦\")\n",
    "logging.info(\"Starting CELL 1 processing pipeline.\")\n",
    "\n",
    "# ======================================================================\n",
    "# 0) CONFIG PATHS\n",
    "# ======================================================================\n",
    "MASTER_DIR = Config.DATA_MASTER_DIR\n",
    "CHUNKS_DIR = Config.DATA_CHUNKS_DIR\n",
    "PROCESSED_DIR = Config.PROCESSED_DIR\n",
    "\n",
    "os.makedirs(PROCESSED_DIR, exist_ok=True)\n",
    "\n",
    "# ======================================================================\n",
    "# 1) REQUIRED SCHEMAS\n",
    "# ======================================================================\n",
    "MASTER_REQUIRED = [\n",
    "    \"service_id\", \"service_title_ar\", \"description_full\",\n",
    "    \"beneficiaries\", \"fees\", \"conditions\", \"access_path\"\n",
    "]\n",
    "\n",
    "CHUNK_REQUIRED = [\n",
    "    \"chunk_id\", \"service_id\", \"chunk_title\",\n",
    "    \"chunk_text\", \"chunk_type\", \"language\", \"meta\"\n",
    "]\n",
    "\n",
    "# ======================================================================\n",
    "# 2) Arabic Normalization + Soft Cleaning\n",
    "# ======================================================================\n",
    "ARABIC_DIAC = re.compile(r\"[\\u0617-\\u061A\\u064B-\\u0652\\u0670\\u06D6-\\u06ED]\")\n",
    "\n",
    "def normalize_arabic(text: str) -> str:\n",
    "    \"\"\"Apply consistent Arabic normalization for RAG stability.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = unicodedata.normalize(\"NFC\", text)\n",
    "    text = ARABIC_DIAC.sub(\"\", text)\n",
    "    text = re.sub(r\"[Ø£Ø¥Ø¢Ù±]\", \"Ø§\", text)\n",
    "    text = text.replace(\"Ù‰\", \"ÙŠ\")\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "def soft_clean(text: str) -> str:\n",
    "    \"\"\"Remove markdown markers and noisy formatting.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = re.sub(r\"\\*{1,2}(.*?)\\*{1,2}\", r\"\\1\", text)\n",
    "    text = re.sub(r\"_+(.*?)_+\", r\"\\1\", text)\n",
    "    text = re.sub(r\"#+\\s*\", \"\", text)\n",
    "    text = re.sub(r\"\\[cite[^\\]]*\\]\", \"\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "def clean_df(df, cols):\n",
    "    for c in cols:\n",
    "        if c in df.columns:\n",
    "            df[c] = df[c].astype(str).map(soft_clean).map(normalize_arabic)\n",
    "    return df\n",
    "\n",
    "# ======================================================================\n",
    "# 3) Sector Inference\n",
    "# ======================================================================\n",
    "SECTOR_MAP = {\n",
    "    \"jawazat\": \"Ø§Ù„Ø¬ÙˆØ§Ø²Ø§Øª\",\n",
    "    \"muroor\": \"Ø§Ù„Ù…Ø±ÙˆØ±\",\n",
    "    \"ahwal\": \"Ø§Ù„Ø£Ø­ÙˆØ§Ù„ Ø§Ù„Ù…Ø¯Ù†ÙŠØ©\",\n",
    "    \"waffedeen\": \"Ø´Ø¤ÙˆÙ† Ø§Ù„ÙˆØ§ÙØ¯ÙŠÙ†\",\n",
    "    \"tafweed\": \"Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„ØªÙØ§ÙˆÙŠØ¶\",\n",
    "    \"prisons\": \"Ø§Ù„Ù…Ø¯ÙŠØ±ÙŠØ© Ø§Ù„Ø¹Ø§Ù…Ø© Ù„Ù„Ø³Ø¬ÙˆÙ†\",\n",
    "    \"amn\": \"Ø§Ù„Ø£Ù…Ù† Ø§Ù„Ø¹Ø§Ù…\",\n",
    "    \"niyaba\": \"Ø§Ù„Ù†ÙŠØ§Ø¨Ø© Ø§Ù„Ø¹Ø§Ù…Ø©\",\n",
    "    \"moiministry\": \"ÙˆØ²Ø§Ø±Ø© Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠØ©\",\n",
    "    \"hajj\": \"ÙˆØ²Ø§Ø±Ø© Ø§Ù„Ø­Ø¬ ÙˆØ§Ù„Ø¹Ù…Ø±Ø©\"\n",
    "}\n",
    "\n",
    "def infer_sector(path: str) -> str:\n",
    "    base = os.path.basename(path).lower()\n",
    "    key = base.split(\"_\")[0]\n",
    "    return SECTOR_MAP.get(key, \"ØºÙŠØ± Ù…Ø­Ø¯Ø¯\")\n",
    "\n",
    "# ======================================================================\n",
    "# 4) Load CSVs\n",
    "# ======================================================================\n",
    "master_files = sorted(glob.glob(os.path.join(MASTER_DIR, \"*.csv\")))\n",
    "chunk_files = sorted(glob.glob(os.path.join(CHUNKS_DIR, \"*.csv\")))\n",
    "\n",
    "print(f\"ğŸ“¦ Master files: {len(master_files)}\")\n",
    "print(f\"ğŸ“¦ Chunk files : {len(chunk_files)}\")\n",
    "\n",
    "if not master_files or not chunk_files:\n",
    "    raise RuntimeError(\"âŒ Missing master/chunk CSV files.\")\n",
    "\n",
    "# ======================================================================\n",
    "# 5) Validate + Read CSVs\n",
    "# ======================================================================\n",
    "def validate(df, schema, name):\n",
    "    missing = [c for c in schema if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"âŒ Missing columns in {name}: {missing}\")\n",
    "\n",
    "masters, chunks = [], []\n",
    "\n",
    "for f in master_files:\n",
    "    df = pd.read_csv(f)\n",
    "    validate(df, MASTER_REQUIRED, f)\n",
    "    df[\"__sector\"] = infer_sector(f)\n",
    "    df[\"__source_file\"] = f\n",
    "    masters.append(df)\n",
    "\n",
    "for f in chunk_files:\n",
    "    df = pd.read_csv(f)\n",
    "    validate(df, CHUNK_REQUIRED, f)\n",
    "    df[\"__sector\"] = infer_sector(f)\n",
    "    df[\"__source_file\"] = f\n",
    "    chunks.append(df)\n",
    "\n",
    "df_master = pd.concat(masters, ignore_index=True)\n",
    "df_chunks = pd.concat(chunks, ignore_index=True)\n",
    "\n",
    "print(f\"ğŸ§¾ Master rows: {len(df_master)}\")\n",
    "print(f\"ğŸ§© Chunk rows : {len(df_chunks)}\")\n",
    "\n",
    "# ======================================================================\n",
    "# 6) Normalize all text columns\n",
    "# ======================================================================\n",
    "df_master = clean_df(df_master, [\n",
    "    \"service_title_ar\",\"description_full\",\"beneficiaries\",\n",
    "    \"fees\",\"conditions\",\"access_path\"\n",
    "])\n",
    "\n",
    "df_chunks = clean_df(df_chunks, [\n",
    "    \"chunk_title\",\"chunk_text\",\"chunk_type\",\"language\",\"meta\"\n",
    "])\n",
    "\n",
    "df_chunks = df_chunks[df_chunks[\"chunk_text\"].str.strip() != \"\"]\n",
    "\n",
    "# ======================================================================\n",
    "# 7) Build Master-Level Documents\n",
    "# ======================================================================\n",
    "def build_master_text(row):\n",
    "    parts = [\n",
    "        f\"Ø§Ø³Ù… Ø§Ù„Ø®Ø¯Ù…Ø©: {row['service_title_ar']}\",\n",
    "        f\"Ø§Ù„ÙˆØµÙ: {row['description_full']}\"\n",
    "    ]\n",
    "    if row[\"beneficiaries\"]: parts.append(f\"Ø§Ù„Ù…Ø³ØªÙÙŠØ¯ÙˆÙ†: {row['beneficiaries']}\")\n",
    "    if row[\"fees\"]: parts.append(f\"Ø§Ù„Ø±Ø³ÙˆÙ…: {row['fees']}\")\n",
    "    if row[\"conditions\"]: parts.append(f\"Ø§Ù„Ø´Ø±ÙˆØ·: {row['conditions']}\")\n",
    "    if row[\"access_path\"]: parts.append(f\"Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„ÙˆØµÙˆÙ„: {row['access_path']}\")\n",
    "    return \" | \".join(parts)\n",
    "\n",
    "service_docs_list = []\n",
    "for (sector, sid), g in df_master.groupby([\"__sector\", \"service_id\"]):\n",
    "    best_row = g.loc[g[\"description_full\"].str.len().idxmax()]\n",
    "    service_docs_list.append({\n",
    "        \"sector\": sector,\n",
    "        \"service_id\": sid,\n",
    "        \"service_title_ar\": best_row[\"service_title_ar\"],\n",
    "        \"text\": build_master_text(best_row),\n",
    "        \"source_files\": list(g[\"__source_file\"].unique())\n",
    "    })\n",
    "\n",
    "df_service_docs = pd.DataFrame(service_docs_list)\n",
    "print(f\"ğŸ§± Service docs built: {len(df_service_docs)}\")\n",
    "\n",
    "processed_service_documents = [\n",
    "    Document(\n",
    "        page_content=r[\"text\"],\n",
    "        metadata={\n",
    "            \"sector\": r[\"sector\"],\n",
    "            \"service_id\": r[\"service_id\"],\n",
    "            \"service_title\": r[\"service_title_ar\"],\n",
    "            \"doc_level\": \"service_master\",\n",
    "            \"source_files\": r[\"source_files\"]\n",
    "        }\n",
    "    )\n",
    "    for _, r in df_service_docs.iterrows()\n",
    "]\n",
    "\n",
    "# ======================================================================\n",
    "# 8) Build Chunk-Level Documents\n",
    "# ======================================================================\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=900,\n",
    "    chunk_overlap=120,\n",
    "    length_function=len\n",
    ")\n",
    "\n",
    "processed_chunks = []\n",
    "\n",
    "for _, row in df_chunks.iterrows():\n",
    "    meta = {\n",
    "        \"sector\": row[\"__sector\"],\n",
    "        \"service_id\": row[\"service_id\"],\n",
    "        \"chunk_id\": row[\"chunk_id\"],\n",
    "        \"chunk_title\": row[\"chunk_title\"],\n",
    "        \"chunk_type\": row[\"chunk_type\"],\n",
    "        \"language\": row[\"language\"],\n",
    "        \"meta\": row[\"meta\"],\n",
    "        \"source_file\": row[\"__source_file\"],\n",
    "        \"doc_level\": \"service_chunk\"\n",
    "    }\n",
    "\n",
    "    text = row[\"chunk_text\"]\n",
    "    if len(text) > 1000:\n",
    "        parts = splitter.split_text(text)\n",
    "        for i, p in enumerate(parts):\n",
    "            m = dict(meta)\n",
    "            m[\"chunk_part\"] = f\"{i+1}/{len(parts)}\"\n",
    "            processed_chunks.append(Document(page_content=p, metadata=m))\n",
    "    else:\n",
    "        processed_chunks.append(Document(page_content=text, metadata=meta))\n",
    "\n",
    "print(f\"âœ‚ï¸ Chunk documents prepared: {len(processed_chunks)}\")\n",
    "\n",
    "# ======================================================================\n",
    "# 9) Save artifacts\n",
    "# ======================================================================\n",
    "preview_path = os.path.join(PROCESSED_DIR, \"service_docs_preview.jsonl\")\n",
    "with open(preview_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for d in processed_service_documents:\n",
    "        f.write(json.dumps({\"text\": d.page_content, \"metadata\": d.metadata}, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "stats_path = os.path.join(PROCESSED_DIR, \"chunk_docs_stats.json\")\n",
    "with open(stats_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\n",
    "        \"num_service_docs\": len(processed_service_documents),\n",
    "        \"num_chunk_docs\": len(processed_chunks),\n",
    "        \"master_files\": master_files,\n",
    "        \"chunk_files\": chunk_files\n",
    "    }, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"ğŸ’¾ Preview saved â†’ {preview_path}\")\n",
    "print(f\"ğŸ’¾ Stats saved   â†’ {stats_path}\")\n",
    "\n",
    "display(HTML(\"<h2 style='text-align:center;color:green;'>âœ… CELL 1 completed successfully</h2>\"))\n",
    "\n",
    "print(\"\\nğŸ‰ CELL 1 COMPLETE.\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78b95b00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”§ Starting CELL 2 â€” Pro RAG Model Initializationâ€¦\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step 1: Embeddings (bge-m3):   0%|           | 0/4 Step 1: Embeddings (bge-m3): "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¹ Loading BGE-M3 embedding model on GPUâ€¦\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step 2: Vector Store (FAISS):  25%|â–ˆâ–ˆâ–Œ        | 1/4 Step 2: Vector Store (FAISS): "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‚ Found existing FAISS index â†’ chatbot_project/vector_db\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step 3: LLM (ALLaM 7B):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ      | 2/4 Step 3: LLM (ALLaM 7B):             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Existing FAISS index loaded & verified.\n",
      "ğŸ”¹ Loading ALLaM 7B (bf16)â€¦\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "216346a26ab040f59a7b2d1b74608966",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step 4: ASR (Whisper Large V3):  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 3/4 Step 4: ASR (Whisper Large V3): "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… LLM ready.\n",
      "ğŸ”¹ Loading Whisper Large V3 (FP32)â€¦\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Device set to use cuda:0\n",
      "Step 4: ASR (Whisper Large V3): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 4/4 Step 4: ASR (Whisper Large V3): "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ…âœ…âœ… CELL 2 COMPLETE â€” Enhanced Models + Vector DB Loaded âœ…âœ…âœ…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# @title ğŸ§  CELL 2 â€” Load AI Models & Build/Load FAISS Vector Store (Pro RAG, Final Version)\n",
    "# English-only comments as requested.\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import logging\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "print(\"ğŸ”§ Starting CELL 2 â€” Pro RAG Model Initializationâ€¦\")\n",
    "logging.info(\"CELL 2 started.\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 0) Pre-run validation\n",
    "# ==============================================================================\n",
    "if \"processed_chunks\" not in globals() or not processed_chunks:\n",
    "    raise RuntimeError(\"âŒ processed_chunks missing â€” run CELL 1 first.\")\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    raise RuntimeError(\"âŒ CUDA GPU not available â€” A100/V100 required.\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 1) Initialize globals\n",
    "# ==============================================================================\n",
    "embedding_model = None\n",
    "llm_tokenizer = None\n",
    "llm_model = None\n",
    "asr_pipeline = None\n",
    "vector_store = None\n",
    "rag_models_and_vector_store_ready = False\n",
    "\n",
    "VECTOR_DIR = Config.VECTOR_DB_DIR\n",
    "os.makedirs(VECTOR_DIR, exist_ok=True)\n",
    "\n",
    "# ==============================================================================\n",
    "# 2) Helper functions: load/build FAISS\n",
    "# ==============================================================================\n",
    "def load_faiss(path, embeddings):\n",
    "    fbin = os.path.join(path, \"index.faiss\")\n",
    "    if not os.path.exists(fbin):\n",
    "        return None\n",
    "    print(f\"ğŸ“‚ Found existing FAISS index â†’ {path}\")\n",
    "\n",
    "    try:\n",
    "        vs = FAISS.load_local(\n",
    "            path,\n",
    "            embeddings,\n",
    "            allow_dangerous_deserialization=True\n",
    "        )\n",
    "        vs.similarity_search(\"Ø§Ø®ØªØ¨Ø§Ø±\", k=1)  # sanity check\n",
    "        print(\"âœ… Existing FAISS index loaded & verified.\")\n",
    "        return vs\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ FAISS load error: {e}\")\n",
    "        print(\"ğŸ—‘ Removing corrupted FAISS directory and rebuildingâ€¦\")\n",
    "        shutil.rmtree(path, ignore_errors=True)\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "        return None\n",
    "\n",
    "\n",
    "def build_faiss(docs, embeddings, path):\n",
    "    print(f\"âš¡ Building FAISS for {len(docs)} documentsâ€¦ (GPU accelerated)\")\n",
    "    vs = FAISS.from_documents(docs, embedding=embeddings)\n",
    "    vs.save_local(path)\n",
    "    print(f\"âœ… New FAISS index saved â†’ {path}\")\n",
    "    return vs\n",
    "\n",
    "# ==============================================================================\n",
    "# 3) Progress bar\n",
    "# ==============================================================================\n",
    "pbar = tqdm(total=4, bar_format=\"{l_bar}{bar} | {n_fmt}/{total_fmt} {desc}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 4) Step 1 â€” BGE-M3 Embeddings\n",
    "# ==============================================================================\n",
    "pbar.set_description(\"Step 1: Embeddings (bge-m3)\")\n",
    "print(\"ğŸ”¹ Loading BGE-M3 embedding model on GPUâ€¦\")\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=Config.EMBEDDING_MODEL_NAME,\n",
    "    cache_folder=Config.MODELS_DIR,\n",
    "    model_kwargs={\"device\": \"cuda\"},\n",
    "    encode_kwargs={\"normalize_embeddings\": True}\n",
    ")\n",
    "\n",
    "# Warmup to avoid first-call latency\n",
    "_ = embedding_model.embed_query(\"Ø§Ø®ØªØ¨Ø§Ø±\")\n",
    "\n",
    "pbar.update(1)\n",
    "\n",
    "# ==============================================================================\n",
    "# 5) Step 2 â€” Load/Build FAISS Vector Store\n",
    "# ==============================================================================\n",
    "pbar.set_description(\"Step 2: Vector Store (FAISS)\")\n",
    "\n",
    "vector_store = load_faiss(VECTOR_DIR, embedding_model)\n",
    "if vector_store is None:\n",
    "    vector_store = build_faiss(processed_chunks, embedding_model, VECTOR_DIR)\n",
    "\n",
    "pbar.update(1)\n",
    "\n",
    "# ==============================================================================\n",
    "# 6) Step 3 â€” Load ALLaM 7B LLM\n",
    "# ==============================================================================\n",
    "pbar.set_description(\"Step 3: LLM (ALLaM 7B)\")\n",
    "\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = Config.MODELS_DIR\n",
    "\n",
    "print(\"ğŸ”¹ Loading ALLaM 7B (bf16)â€¦\")\n",
    "llm_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    Config.LLM_MODEL_NAME,\n",
    "    cache_dir=Config.MODELS_DIR,\n",
    "    use_fast=False\n",
    ")\n",
    "\n",
    "llm_model = AutoModelForCausalLM.from_pretrained(\n",
    "    Config.LLM_MODEL_NAME,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    cache_dir=Config.MODELS_DIR,\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "\n",
    "print(\"âœ… LLM ready.\")\n",
    "pbar.update(1)\n",
    "\n",
    "# ==============================================================================\n",
    "# 7) Step 4 â€” Load Whisper Large V3 (No Warmup / No Bytes)\n",
    "# ==============================================================================\n",
    "pbar.set_description(\"Step 4: ASR (Whisper Large V3)\")\n",
    "\n",
    "print(\"ğŸ”¹ Loading Whisper Large V3 (FP32)â€¦\")\n",
    "\n",
    "asr_pipeline = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=Config.ASR_MODEL_NAME,\n",
    "    device=\"cuda:0\",\n",
    "    torch_dtype=torch.float32,\n",
    "    model_kwargs={\n",
    "        \"cache_dir\": Config.MODELS_DIR,\n",
    "        \"attn_implementation\": \"sdpa\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# âœ… No warmup needed â€” avoids ValueError from invalid audio input.\n",
    "\n",
    "pbar.update(1)\n",
    "pbar.close()\n",
    "\n",
    "# ==============================================================================\n",
    "# 8) Final confirmation\n",
    "# ==============================================================================\n",
    "rag_models_and_vector_store_ready = True\n",
    "\n",
    "print(\"\\nâœ…âœ…âœ… CELL 2 COMPLETE â€” Enhanced Models + Vector DB Loaded âœ…âœ…âœ…\")\n",
    "logging.info(\"CELL 2 complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b9dad07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§  Building PRO RAG chain (v2 â€” Post-hoc Translation)â€¦\n",
      "âœ… PRO RAG Chain (v2) is ready.\n"
     ]
    }
   ],
   "source": [
    "# @title ğŸ”— CELL 4 â€” Pro RAG Chain (Rewrite â†’ Retrieve â†’ Rerank â†’ Answer) [FIXED]\n",
    "# English-only comments as requested.\n",
    "\n",
    "import re\n",
    "import logging\n",
    "import warnings\n",
    "from typing import List, Tuple, Dict, Any\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from langchain.schema import Document\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from transformers import pipeline\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(\"ğŸ§  Building PRO RAG chain (v2 â€” Post-hoc Translation)â€¦\")\n",
    "logging.info(\"CELL 4 (Fixed) started.\")\n",
    "\n",
    "# ---------------- Safety Checks ----------------\n",
    "if \"rag_models_and_vector_store_ready\" not in globals() or not rag_models_and_vector_store_ready:\n",
    "    raise RuntimeError(\"âŒ CELL 2 not ready.\")\n",
    "if \"processed_chunks\" not in globals() or not processed_chunks:\n",
    "    raise RuntimeError(\"âŒ CELL 1 not ready.\")\n",
    "\n",
    "# ---------------- Arabic Normalization ----------------\n",
    "ARABIC_DIAC = re.compile(r\"[\\u0617-\\u061A\\u064B-\\u0652\\u0670\\u06D6-\\u06ED]\")\n",
    "\n",
    "def normalize_ar(text: str) -> str:\n",
    "    if not isinstance(text, str): return \"\"\n",
    "    t = ARABIC_DIAC.sub(\"\", text)\n",
    "    t = re.sub(r\"[Ø£Ø¥Ø¢Ù±]\", \"Ø§\", t)\n",
    "    t = t.replace(\"Ù‰\", \"ÙŠ\")\n",
    "    return re.sub(r\"\\s+\", \" \", t).strip()\n",
    "\n",
    "def looks_english(s: str):\n",
    "    return bool(re.search(r\"[A-Za-z]\", s))\n",
    "\n",
    "# â­ï¸ NEW HELPER: Check for Arabic text\n",
    "def is_arabic(text: str) -> bool:\n",
    "    \"\"\"Check if the text contains Arabic characters.\"\"\"\n",
    "    return bool(re.search(r'[\\u0600-\\u06FF]', text))\n",
    "\n",
    "# ---------------- Synonym Expansion ----------------\n",
    "SYNONYMS = {\n",
    "    \"Ø¬ÙˆØ§Ø²\": [\"Ø¬ÙˆØ§Ø² Ø§Ù„Ø³ÙØ±\", \"ØªØ¬Ø¯ÙŠØ¯ Ø§Ù„Ø¬ÙˆØ§Ø²\", \"passport\", \"renew passport\"],\n",
    "    \"Ù‡ÙˆÙŠØ©\": [\"Ø§Ù„Ù‡ÙˆÙŠØ© Ø§Ù„ÙˆØ·Ù†ÙŠØ©\", \"Ø¨Ø·Ø§Ù‚Ø© Ø§Ù„Ø£Ø­ÙˆØ§Ù„\", \"national id\"],\n",
    "    \"Ù…Ù‚ÙŠÙ…\": [\"Ø§Ù„Ø¥Ù‚Ø§Ù…Ø©\", \"Ù‡ÙˆÙŠØ© Ù…Ù‚ÙŠÙ…\", \"iqama\"],\n",
    "    \"Ø²ÙŠØ§Ø±Ø©\": [\"visit visa\", \"ØªÙ…Ø¯ÙŠØ¯ Ø§Ù„Ø²ÙŠØ§Ø±Ø©\"],\n",
    "    \"Ø£Ø¨Ø´Ø±\": [\"absher\", \"Ù…Ù†ØµØ© Ø§Ø¨Ø´Ø±\"],\n",
    "}\n",
    "\n",
    "def expand_synonyms(q: str) -> List[str]:\n",
    "    out = [q]\n",
    "    for key, vals in SYNONYMS.items():\n",
    "        if key in q:\n",
    "            out.extend(vals)\n",
    "    # dedupe\n",
    "    seen, final = set(), []\n",
    "    for t in out:\n",
    "        if t not in seen:\n",
    "            final.append(t); seen.add(t)\n",
    "    return final\n",
    "\n",
    "# ---------------- ENâ†’AR rewriting ----------------\n",
    "def rewrite_en_to_ar(q: str) -> str:\n",
    "    try:\n",
    "        # Use a greedy pipeline for EN->AR rewrite\n",
    "        gen = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=llm_model,\n",
    "            tokenizer=llm_tokenizer,\n",
    "            max_new_tokens=64,\n",
    "            do_sample=False, # Greedy\n",
    "            pad_token_id=llm_tokenizer.eos_token_id\n",
    "        )\n",
    "        prompt = f\"Translate to Arabic (query style):\\nEnglish: {q}\\nArabic:\"\n",
    "        out = gen(prompt)[0][\"generated_text\"]\n",
    "        return normalize_ar(out.split(\"Arabic:\", 1)[-1])\n",
    "    except:\n",
    "        return q\n",
    "\n",
    "def rewrite_and_expand(q: str) -> Tuple[str, List[str]]:\n",
    "    if looks_english(q):\n",
    "        q_ar = rewrite_en_to_ar(q)\n",
    "    else:\n",
    "        q_ar = normalize_ar(q)\n",
    "    expanded = expand_synonyms(q_ar.lower())\n",
    "    return q_ar, expanded\n",
    "\n",
    "# ---------------- Hybrid Retrieval ----------------\n",
    "bm25 = BM25Retriever.from_documents(processed_chunks)\n",
    "bm25.k = 12\n",
    "\n",
    "dense = vector_store.as_retriever(search_kwargs={\"k\": 12})\n",
    "\n",
    "def rrf_merge(a: List[Document], b: List[Document], k=60) -> List[Document]:\n",
    "    scores, store = {}, {}\n",
    "    def key(d): return f\"{d.metadata.get('service_id','')}::{d.page_content[:120]}\"\n",
    "    for r,d in enumerate(a,1):\n",
    "        K = key(d); scores[K]=scores.get(K,0)+1/(k+r); store[K]=d\n",
    "    for r,d in enumerate(b,1):\n",
    "        K = key(d); scores[K]=scores.get(K,0)+1/(k+r); store[K]=d\n",
    "    ordered = sorted(scores, key=lambda x: scores[x], reverse=True)\n",
    "    return [store[k] for k in ordered]\n",
    "\n",
    "# ---------------- Dense Re-rank ----------------\n",
    "import numpy as np\n",
    "def cosine(a,b):\n",
    "    a=np.asarray(a); b=np.asarray(b)\n",
    "    denom=(np.linalg.norm(a)*np.linalg.norm(b)) or 1e-9\n",
    "    return float(np.dot(a,b)/denom)\n",
    "\n",
    "def dense_rerank(query, docs, top_k=6):\n",
    "    qv = embedding_model.embed_query(query)\n",
    "    scored=[]\n",
    "    for d in docs[:24]:\n",
    "        dv = embedding_model.embed_query(d.page_content)\n",
    "        scored.append((cosine(qv,dv),d))\n",
    "    scored.sort(key=lambda x: x[0], reverse=True)\n",
    "    return [d for _,d in scored[:top_k]]\n",
    "\n",
    "# ---------------- Intent classification ----------------\n",
    "def detect_intent(q: str) -> str:\n",
    "    nq = normalize_ar(q)\n",
    "    if \"Ø¬ÙˆØ§Ø²\" in nq or \"passport\" in nq: return \"passport\"\n",
    "    if \"Ù…Ù‚ÙŠÙ…\" in nq or \"Ø§Ù‚Ø§Ù…Ø©\" in nq or \"iqama\" in nq: return \"iqama\"\n",
    "    if \"Ù‡ÙˆÙŠØ©\" in nq and \"Ù…Ù‚ÙŠÙ…\" not in nq: return \"national_id\"\n",
    "    return \"generic\"\n",
    "\n",
    "# ---------------- Domain filtering ----------------\n",
    "def domain_filter(docs: List[Document], intent: str) -> List[Document]:\n",
    "    out=[]\n",
    "    for d in docs:\n",
    "        t = normalize_ar(d.page_content)\n",
    "        if intent==\"passport\" and (\"Ø§Ù‚Ø§Ù…Ø©\" in t or \"Ù…Ù‚ÙŠÙ…\" in t): continue\n",
    "        if intent==\"iqama\" and (\"Ù‡ÙˆÙŠØ© ÙˆØ·Ù†ÙŠØ©\" in t or \"Ø§Ù„Ø§Ø­ÙˆØ§Ù„\" in t): continue\n",
    "        if intent==\"national_id\" and (\"Ø§Ù‚Ø§Ù…Ø©\" in t or \"Ù…Ù‚ÙŠÙ…\" in t): continue\n",
    "        out.append(d)\n",
    "    return out if out else docs\n",
    "\n",
    "# ---------------- Language preference (session-level) ----------------\n",
    "class LangPref:\n",
    "    def __init__(self): self.pref=\"auto\"\n",
    "    def update(self, q: str):\n",
    "        ql = q.lower()\n",
    "        if any(t in ql for t in [\"speak in english\",\"answer in english\",\"english please\",\"in english\"]):\n",
    "            self.pref=\"en\"\n",
    "        elif any(t in ql for t in [\"Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠ\",\"Ø¹Ø±Ø¨ÙŠ\",\"Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©\"]):\n",
    "            self.pref=\"ar\"\n",
    "\n",
    "    def final(self, q: str):\n",
    "        if self.pref!=\"auto\": return self.pref\n",
    "        return \"en\" if looks_english(q) else \"ar\"\n",
    "\n",
    "lang_pref = LangPref()\n",
    "\n",
    "# ---------------- System Prompt ----------------\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "<s>[INST] <<SYS>>\n",
    "Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ø°ÙƒÙŠ Ù„Ø®Ø¯Ù…Ø§Øª ÙˆØ²Ø§Ø±Ø© Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠØ© ÙˆÙ…Ù†ØµØ© Ø£Ø¨Ø´Ø±.\n",
    "\n",
    "Ø§Ù„Ø³ÙŠØ§Ø³Ø§Øª:\n",
    "- Ø£Ø¬Ø¨ Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø¥Ù„Ø§ Ø¥Ø°Ø§ Ø·Ù„Ø¨ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ© Ø£Ùˆ Ø³Ø£Ù„ Ø¨Ù‡Ø§.\n",
    "- Ù„Ø§ ØªØ°ÙƒØ± Ø±Ø³ÙˆÙ…/Ø´Ø±ÙˆØ·/Ù…Ø¯Ø¯ Ø¥Ù„Ø§ Ø¥Ø°Ø§ Ø¸Ù‡Ø±Øª ØµØ±Ø§Ø­Ø© ÙÙŠ \"Ø§Ù„Ø³ÙŠØ§Ù‚\".\n",
    "- Ø¥Ù† ÙƒØ§Ù† Ø§Ù„Ø³ÙŠØ§Ù‚ ØºÙŠØ± ÙƒØ§ÙÙØŒ Ù‚Ù„: \"Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø© ØºÙŠØ± Ù…ØªÙˆÙØ±Ø© ÙÙŠ Ø§Ù„Ù…Ø³ØªÙ†Ø¯.\"\n",
    "- ÙŠØ³Ù…Ø­ Ø¨Ø§Ù„Ø¯Ø±Ø¯Ø´Ø© Ø§Ù„Ø®ÙÙŠÙØ© Ø¨Ù†Ø¨Ø±Ø© Ø³Ø¹ÙˆØ¯ÙŠØ© Ù„Ø¨Ù‚Ø©.\n",
    "- ÙŠÙ…Ù†Ø¹ Ø®Ù„Ø·:\n",
    "  â€¢ Ø§Ù„Ù‡ÙˆÙŠØ© Ø§Ù„ÙˆØ·Ù†ÙŠØ© (Ø£Ø­ÙˆØ§Ù„) â‰  Ù‡ÙˆÙŠØ© Ù…Ù‚ÙŠÙ… / Ø§Ù„Ø¥Ù‚Ø§Ù…Ø©.\n",
    "<</SYS>>\n",
    "\n",
    "[Context]\n",
    "{context}\n",
    "\n",
    "[User Question]\n",
    "{question}\n",
    "[/INST]\n",
    "\"\"\"\n",
    "\n",
    "QA_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"context\",\"question\"],\n",
    "    template=SYSTEM_PROMPT\n",
    ")\n",
    "\n",
    "# ---------------- HF Generation Pipeline ----------------\n",
    "llm_pipe = HuggingFacePipeline(\n",
    "    pipeline=pipeline(\n",
    "        \"text-generation\",\n",
    "        model=llm_model,\n",
    "        tokenizer=llm_tokenizer,\n",
    "        max_new_tokens=700,\n",
    "        temperature=0.55,\n",
    "        top_p=0.92,\n",
    "        do_sample=True,\n",
    "        repetition_penalty=1.10,\n",
    "        pad_token_id=llm_tokenizer.eos_token_id\n",
    "    )\n",
    ")\n",
    "\n",
    "def clean_out(txt): return txt.split(\"[/INST]\",1)[-1].strip()\n",
    "\n",
    "# ---------------- â­ï¸ NEW Translation Pipeline ----------------\n",
    "# We create a dedicated, non-sampling (greedy) pipeline for translation\n",
    "try:\n",
    "    translation_pipeline = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=llm_model,\n",
    "        tokenizer=llm_tokenizer,\n",
    "        max_new_tokens=1024, # Allow longer translations\n",
    "        do_sample=False,    # MUST be False for accurate translation\n",
    "        pad_token_id=llm_tokenizer.eos_token_id\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ Warning: Could not create translation pipeline: {e}\")\n",
    "    translation_pipeline = llm_pipe # Fallback to main pipe\n",
    "\n",
    "def post_hoc_translate(text_to_translate: str, target_lang: str) -> str:\n",
    "    \"\"\"Translate text to the target language using the greedy LLM pipeline.\"\"\"\n",
    "    \n",
    "    if target_lang == \"en\":\n",
    "        prompt = f\"Translate this Arabic text to English. Provide *only* the translated text.\\nArabic: {text_to_translate}\\nEnglish:\"\n",
    "    else:\n",
    "        prompt = f\"Translate this English text to Arabic. Provide *only* the translated text.\\nEnglish: {text_to_translate}\\nArabic:\"\n",
    "\n",
    "    try:\n",
    "        raw_trans = translation_pipeline(prompt)[0][\"generated_text\"]\n",
    "        \n",
    "        if target_lang == \"en\":\n",
    "            cleaned = raw_trans.split(\"English:\", 1)[-1].strip()\n",
    "        else:\n",
    "            cleaned = raw_trans.split(\"Arabic:\", 1)[-1].strip()\n",
    "        \n",
    "        # Fallback cleaning if the model includes the prompt\n",
    "        if not cleaned or \"translate this\" in cleaned.lower():\n",
    "             cleaned = raw_trans.splitlines()[-1].strip()\n",
    "\n",
    "        return cleaned if cleaned else text_to_translate\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"Post-hoc translation failed: {e}\")\n",
    "        return text_to_translate # Return original on error\n",
    "\n",
    "# ---------------- Main RAG Class (Updated) ----------------\n",
    "class ProRAGChain:\n",
    "    def __init__(self): self.last_sources=[]\n",
    "\n",
    "    def _ctx(self, docs):\n",
    "        blocks=[]\n",
    "        for d in docs[:6]:\n",
    "            title = d.metadata.get(\"chunk_title\") or d.metadata.get(\"service_title\")\n",
    "            sid   = d.metadata.get(\"service_id\",\"\")\n",
    "            blocks.append(f\"(service_id={sid}) {title}\\n{d.page_content}\")\n",
    "        return \"\\n\\n---\\n\\n\".join(blocks)\n",
    "\n",
    "    def invoke(self, inputs):\n",
    "        q = inputs.get(\"question\",\"\")\n",
    "\n",
    "        # 1. Update language preference\n",
    "        lang_pref.update(q)\n",
    "        final_lang = lang_pref.final(q)\n",
    "\n",
    "        # 2. Rewrite + expand\n",
    "        q_ar, expanded = rewrite_and_expand(q)\n",
    "\n",
    "        # 3. Retrieval\n",
    "        dense_docs = dense.get_relevant_documents(q_ar)\n",
    "        bm_docs    = bm25.get_relevant_documents(\" \".join(expanded))\n",
    "        merged     = rrf_merge(dense_docs, bm_docs)\n",
    "\n",
    "        # 4. Intent + domain filtering\n",
    "        intent = detect_intent(q)\n",
    "        merged = domain_filter(merged, intent)\n",
    "\n",
    "        # 5. Re-rank\n",
    "        final_docs = dense_rerank(q_ar, merged, top_k=6)\n",
    "        self.last_sources = final_docs\n",
    "\n",
    "        # 6. Generate Answer\n",
    "        lang_tag = \"Answer in English.\" if final_lang==\"en\" else \"Ø£Ø¬Ø¨ Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©.\"\n",
    "        prompt = QA_PROMPT.format(\n",
    "            context=self._ctx(final_docs),\n",
    "            question=f\"{q}\\n\\n{lang_tag}\"\n",
    "        )\n",
    "\n",
    "        raw = llm_pipe.invoke(prompt)\n",
    "        ans = clean_out(raw)\n",
    "\n",
    "        # 7. â­ï¸ NEW: Post-hoc Translation Check â­ï¸\n",
    "        ans_is_arabic = is_arabic(ans)\n",
    "        \n",
    "        if final_lang == \"en\" and ans_is_arabic:\n",
    "            logging.info(\"Post-hoc translation required: AR -> EN\")\n",
    "            ans = post_hoc_translate(ans, \"en\")\n",
    "        \n",
    "        elif final_lang == \"ar\" and not ans_is_arabic and looks_english(ans):\n",
    "            # Also handle the reverse case (e.g., chitchat in EN)\n",
    "            logging.info(\"Post-hoc translation required: EN -> AR\")\n",
    "            ans = post_hoc_translate(ans, \"ar\")\n",
    "\n",
    "        return {\"answer\": ans}\n",
    "\n",
    "qa_chain = ProRAGChain()\n",
    "print(\"âœ… PRO RAG Chain (v2) is ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0abf6cfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Launching MOI Branded Interface...\n",
      "âœ… Launching MOI App...\n",
      "Running on local URL:  http://127.0.0.1:7860\n",
      "IMPORTANT: You are using gradio version 3.50.2, however version 4.44.1 is available, please upgrade.\n",
      "--------\n",
      "Running on public URL: https://a254b64afd33279a25.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# @title ğŸ¤– CELL 5 â€” Pro Chat UI (MOI Edition - Gradio 3.50 Stable) ğŸ‡¸ğŸ‡¦\n",
    "# Replaces ipywidgets with a modern, multi-page Gradio Web App.\n",
    "\n",
    "import gradio as gr\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Disable tokenizers parallelism to avoid deadlocks\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "print(\"ğŸš€ Launching MOI Branded Interface...\")\n",
    "\n",
    "# =====================================================\n",
    "# 1. Custom CSS (MOI Theme)\n",
    "# =====================================================\n",
    "moi_css = \"\"\"\n",
    ".gradio-container { background-color: #f4f6f8 !important; }\n",
    ".moi-header {\n",
    "    text-align: center;\n",
    "    padding: 30px;\n",
    "    background: linear-gradient(90deg, #006C35 0%, #004D26 100%);\n",
    "    border-radius: 10px;\n",
    "    color: white;\n",
    "    margin-bottom: 20px;\n",
    "    box-shadow: 0 4px 15px rgba(0,0,0,0.2);\n",
    "}\n",
    ".moi-header h1 { color: white !important; font-size: 2.5em; margin: 0; }\n",
    ".moi-header p { color: #d4af37 !important; font-size: 1.2em; margin-top: 5px; }\n",
    ".lang-btn { font-size: 1.2em; height: 60px; border-radius: 8px !important; }\n",
    "\"\"\"\n",
    "\n",
    "# =====================================================\n",
    "# 2. Logic Handlers\n",
    "# =====================================================\n",
    "def start_chat(lang):\n",
    "    \"\"\"Transition from Welcome Screen to Chat Screen\"\"\"\n",
    "    # Configure UI based on language choice\n",
    "    if lang == \"Arabic\":\n",
    "        greeting = [(None, \"ğŸ‘‹ Ø­ÙŠØ§Ùƒ Ø§Ù„Ù„Ù‡! Ø£Ù†Ø§ Ù…Ø³Ø§Ø¹Ø¯Ùƒ Ø§Ù„Ø°ÙƒÙŠ Ù„Ø®Ø¯Ù…Ø§Øª ÙˆØ²Ø§Ø±Ø© Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠØ©. ØªÙØ¶Ù„ Ø¨Ø·Ø±Ø­ Ø³Ø¤Ø§Ù„Ùƒ.\")]\n",
    "        rtl = True\n",
    "        label = \"Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ø§Ù„ÙÙˆØ±ÙŠØ©\"\n",
    "        placeholder = \"Ø§ÙƒØªØ¨ Ø³Ø¤Ø§Ù„Ùƒ Ù‡Ù†Ø§...\"\n",
    "    else:\n",
    "        greeting = [(None, \"ğŸ‘‹ Hello! I am your MOI Smart Assistant. How can I help you today?\")]\n",
    "        rtl = False\n",
    "        label = \"Live Chat\"\n",
    "        placeholder = \"Type your question here...\"\n",
    "        \n",
    "    return (\n",
    "        gr.update(visible=False),           # Hide Welcome Screen\n",
    "        gr.update(visible=True),            # Show Chat Screen\n",
    "        greeting,                           # Set History\n",
    "        lang,                               # Set State\n",
    "        gr.update(label=label, rtl=rtl),    # Update Chatbot Label/Dir\n",
    "        gr.update(placeholder=placeholder, rtl=rtl) # Update Input Dir\n",
    "    )\n",
    "\n",
    "def chat_response(message, history, audio_file, lang_val):\n",
    "    \"\"\"Handles text/audio input and calls the backend chain.\"\"\"\n",
    "    \n",
    "    # 1. Handle Audio (Priority)\n",
    "    if audio_file:\n",
    "        try:\n",
    "            target_lang = \"ar\" if lang_val == \"Arabic\" else \"en\"\n",
    "            # Check if Whisper is loaded\n",
    "            if 'asr_pipe' in globals() and asr_pipe:\n",
    "                text = asr_pipe(audio_file, generate_kwargs={\"language\": target_lang})[\"text\"].strip()\n",
    "                message = text\n",
    "                user_display = f\"ğŸ¤ {text}\"\n",
    "            else:\n",
    "                return history + [[None, \"âš ï¸ Error: Whisper model not loaded.\"]]\n",
    "        except Exception as e:\n",
    "            return history + [[None, f\"âŒ Audio Error: {str(e)}\"]]\n",
    "    else:\n",
    "        user_display = message\n",
    "\n",
    "    # 2. Validate Input\n",
    "    if not message: return history\n",
    "\n",
    "    # 3. Call the Brain (Hybrid Chain)\n",
    "    try:\n",
    "        # Support both naming conventions (Legacy vs Golden)\n",
    "        if 'hybrid_chain' in globals():\n",
    "            response = hybrid_chain.answer(message)\n",
    "        elif 'qa_chain' in globals():\n",
    "            # Fallback to legacy chain if hybrid isn't ready\n",
    "            res = qa_chain.invoke({\"question\": message})\n",
    "            response = res.get(\"answer\", \"\")\n",
    "        else:\n",
    "            response = \"âš ï¸ System Warning: AI Brain not loaded. Please run previous cells.\"\n",
    "            \n",
    "    except Exception as e:\n",
    "        response = f\"âŒ System Error: {e}\"\n",
    "\n",
    "    # 4. Update History\n",
    "    history.append((user_display, response))\n",
    "    return history\n",
    "\n",
    "def reset_app():\n",
    "    \"\"\"Return to language selection\"\"\"\n",
    "    return (\n",
    "        gr.update(visible=True),  # Show Welcome\n",
    "        gr.update(visible=False), # Hide Chat\n",
    "        None,                     # Clear State\n",
    "        []                        # Clear History\n",
    "    )\n",
    "\n",
    "def clear_inputs(): return \"\", None\n",
    "\n",
    "# =====================================================\n",
    "# 3. Layout (Blocks)\n",
    "# =====================================================\n",
    "with gr.Blocks(theme=gr.themes.Soft(), css=moi_css, title=\"MOI Assistant\") as demo:\n",
    "    \n",
    "    # State to hold language preference\n",
    "    lang_state = gr.State(value=\"Arabic\")\n",
    "\n",
    "    # --- Header ---\n",
    "    gr.HTML(\"\"\"\n",
    "    <div class='moi-header'>\n",
    "        <h1>Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯ Ø§Ù„Ø°ÙƒÙŠ Ù„ÙˆØ²Ø§Ø±Ø© Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠØ©</h1>\n",
    "        <p>MOI Smart Assistant | Powered by ALLaM-7B</p>\n",
    "    </div>\n",
    "    \"\"\")\n",
    "    \n",
    "    # --- SCREEN 1: Welcome & Language Selection ---\n",
    "    with gr.Group(visible=True) as welcome_screen:\n",
    "        gr.Markdown(\n",
    "            \"<h3 style='text-align: center;'>ğŸŒ Please select your preferred language / Ø§Ù„Ø±Ø¬Ø§Ø¡ Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ù„ØºØ© Ø§Ù„Ù…ÙØ¶Ù„Ø©</h3>\"\n",
    "        )\n",
    "        with gr.Row():\n",
    "            btn_ar = gr.Button(\"Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ğŸ‡¸ğŸ‡¦\", variant=\"primary\", elem_classes=[\"lang-btn\"])\n",
    "            btn_en = gr.Button(\"English ğŸ‡¬ğŸ‡§\", variant=\"secondary\", elem_classes=[\"lang-btn\"])\n",
    "\n",
    "    # --- SCREEN 2: Chat Interface ---\n",
    "    with gr.Group(visible=False) as chat_screen:\n",
    "        with gr.Row():\n",
    "            # Left: Chatbot Window\n",
    "            with gr.Column(scale=3):\n",
    "                chatbot = gr.Chatbot(label=\"Chat\", height=500)\n",
    "                with gr.Row():\n",
    "                    msg = gr.Textbox(show_label=False, container=False, scale=4)\n",
    "                    submit_btn = gr.Button(\"ğŸš€\", variant=\"primary\", scale=1)\n",
    "\n",
    "            # Right: Settings & Audio\n",
    "            with gr.Column(scale=1):\n",
    "                gr.Markdown(\"### âš™ï¸ Ø£Ø¯ÙˆØ§Øª / Tools\")\n",
    "                \n",
    "                # Audio Input\n",
    "                audio_input = gr.Audio(source=\"microphone\", type=\"filepath\", label=\"ØªØ³Ø¬ÙŠÙ„ ØµÙˆØªÙŠ / Voice Input\")\n",
    "                \n",
    "                gr.Markdown(\"---\")\n",
    "                restart_btn = gr.Button(\"ğŸ”„ ØªØºÙŠÙŠØ± Ø§Ù„Ù„ØºØ© / Change Language\", variant=\"secondary\")\n",
    "\n",
    "    # =====================================================\n",
    "    # 4. Event Wiring\n",
    "    # =====================================================\n",
    "    \n",
    "    # Language Selection Events\n",
    "    btn_ar.click(\n",
    "        fn=lambda: start_chat(\"Arabic\"),\n",
    "        outputs=[welcome_screen, chat_screen, chatbot, lang_state, chatbot, msg]\n",
    "    )\n",
    "    btn_en.click(\n",
    "        fn=lambda: start_chat(\"English\"),\n",
    "        outputs=[welcome_screen, chat_screen, chatbot, lang_state, chatbot, msg]\n",
    "    )\n",
    "\n",
    "    # Chat Events (Text & Audio)\n",
    "    msg.submit(chat_response, [msg, chatbot, audio_input, lang_state], [chatbot]) \\\n",
    "       .then(clear_inputs, None, [msg, audio_input])\n",
    "    \n",
    "    submit_btn.click(chat_response, [msg, chatbot, audio_input, lang_state], [chatbot]) \\\n",
    "              .then(clear_inputs, None, [msg, audio_input])\n",
    "\n",
    "    # Restart Event\n",
    "    restart_btn.click(reset_app, outputs=[welcome_screen, chat_screen, lang_state, chatbot])\n",
    "\n",
    "# =====================================================\n",
    "# 5. Launch\n",
    "# =====================================================\n",
    "print(\"âœ… Launching MOI App...\")\n",
    "# share=True creates a public link for mobile testing\n",
    "demo.queue().launch(share=True, inline=False, show_api=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cafc9df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
