{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52d7fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title ‚öôÔ∏è CELL 0 ‚Äî Configuration & Environment Setup (The Golden Copy)\n",
    "# English-only comments as requested.\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import warnings\n",
    "import torch\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "# --- 1. Suppress Non-Critical Warnings ---\n",
    "# This keeps the notebook output clean and professional.\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "# Suppress specific LangChain deprecation warnings for cleaner logs\n",
    "warnings.filterwarnings(\"ignore\", module=\"langchain\")\n",
    "\n",
    "print(\"üöÄ Initializing Golden Copy Environment...\")\n",
    "\n",
    "# --- 2. GPU Validation ---\n",
    "if not torch.cuda.is_available():\n",
    "    raise RuntimeError(\"‚ùå No GPU detected! This notebook requires an A100/V100 GPU.\")\n",
    "    \n",
    "gpu_name = torch.cuda.get_device_name(0)\n",
    "print(f\"‚úÖ GPU Detected: {gpu_name}\")\n",
    "\n",
    "# --- 3. Central Configuration Class ---\n",
    "class Config:\n",
    "    # =====================================================\n",
    "    # üèóÔ∏è PATHS (Ibex Cluster Specific)\n",
    "    # =====================================================\n",
    "    \n",
    "    # Base Project Directory\n",
    "    PROJECT_DIR = Path(\"/ibex/user/rashidah/projects/MOI_ChatBot/chatbot_project\")\n",
    "    \n",
    "    # üî¥ CRITICAL: Pointing to the SFT MERGED Model (From Notebook 2)\n",
    "    LLM_MODEL_PATH = Path(\"/ibex/user/rashidah/projects/MOI_ChatBot/al-lam_bilingual_sft/artifacts/ALLaM-7B-MOI-Bilingual-Merged\")\n",
    "    \n",
    "    # External Data Source (The CSVs)\n",
    "    DATA_MASTER_DIR = PROJECT_DIR / \"1_data\" / \"Data_Master\"\n",
    "    DATA_CHUNKS_DIR = PROJECT_DIR / \"1_data\" / \"Data_chunks\"\n",
    "    \n",
    "    # Internal Artifacts (Where we save Vector DB & Logs)\n",
    "    VECTOR_DB_DIR = PROJECT_DIR / \"vector_db_hybrid\"  # New folder for the Hybrid DB\n",
    "    LOGS_DIR = PROJECT_DIR / \"4_outputs\" / \"logs\"\n",
    "    \n",
    "    # =====================================================\n",
    "    # üß† MODEL SETTINGS\n",
    "    # =====================================================\n",
    "    EMBEDDING_MODEL_NAME = \"BAAI/bge-m3\"\n",
    "    \n",
    "    # üé§ ASR (Audio) Settings - [ADDED IMPROVEMENT]\n",
    "    ASR_MODEL_NAME = \"openai/whisper-large-v3\"\n",
    "    \n",
    "    # RAG Retrieval Settings\n",
    "    RETRIEVAL_K = 8        # Fetch more docs initially (High Recall)\n",
    "    RERANK_TOP_K = 5       # Filter down to the best 5 (High Precision)\n",
    "    \n",
    "    # Generation Settings (SFT Tuned)\n",
    "    GEN_MAX_TOKENS = 1024\n",
    "    GEN_TEMP = 0.3         # Low temperature for factual consistency\n",
    "    GEN_REP_PENALTY = 1.15 # Slightly increased to kill loop repetition\n",
    "    \n",
    "    # =====================================================\n",
    "    # üìù LOGGING\n",
    "    # =====================================================\n",
    "    LOG_FILE = LOGS_DIR / \"golden_app.log\"\n",
    "\n",
    "# --- 4. Directory Initialization ---\n",
    "for d in [Config.VECTOR_DB_DIR, Config.LOGS_DIR]:\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "# --- 5. Logger Setup ---\n",
    "logging.basicConfig(\n",
    "    filename=Config.LOG_FILE,\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - [HybridBot] - %(levelname)s - %(message)s\",\n",
    "    force=True\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Configuration Loaded.\")\n",
    "print(f\"üß† Using SFT Model: {Config.LLM_MODEL_PATH.name}\")\n",
    "print(f\"üé§ Using ASR Model: {Config.ASR_MODEL_NAME}\")\n",
    "print(f\"üìÇ Logs path: {Config.LOG_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0ffcae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title üìä CELL 1 ‚Äî Data Processing & Vector DB (The Ultimate Memory: CSV + Master + SFT) üß†\n",
    "# English-only comments as requested.\n",
    "\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import json\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "print(\"üìä Starting Ultimate Data Pipeline...\")\n",
    "\n",
    "# üî¥ UPDATED PATH: SFT Data Location\n",
    "SFT_DATA_PATH = \"/ibex/user/rashidah/projects/MOI_ChatBot/chatbot_project/2_processed/bilingual_moi_absher_sFT_v2.jsonl\"\n",
    "\n",
    "# =====================================================\n",
    "# 1. Text Normalization\n",
    "# =====================================================\n",
    "def normalize_ar(text):\n",
    "    if not isinstance(text, str): return \"\"\n",
    "    text = unicodedata.normalize(\"NFC\", text)\n",
    "    text = re.sub(r\"[\\u0617-\\u061A\\u064B-\\u0652\\u0670\\u06D6-\\u06ED]\", \"\", text)\n",
    "    text = re.sub(r\"[ÿ£ÿ•ÿ¢Ÿ±]\", \"ÿß\", text)\n",
    "    text = text.replace(\"Ÿâ\", \"Ÿä\")\n",
    "    return re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "processed_docs = []\n",
    "\n",
    "# =====================================================\n",
    "# 2. Phase A: Master Mapping (To fix generic titles)\n",
    "# =====================================================\n",
    "print(\"üîπ [Phase A] Building Service Name Map from Masters...\")\n",
    "master_files = sorted(glob.glob(os.path.join(Config.DATA_MASTER_DIR, \"*.csv\")))\n",
    "df_master = pd.concat([pd.read_csv(f) for f in master_files], ignore_index=True)\n",
    "\n",
    "service_map = {}\n",
    "for _, row in df_master.iterrows():\n",
    "    sid = str(row.get(\"service_id\", \"\")).strip()\n",
    "    title = normalize_ar(str(row.get(\"service_title_ar\", \"\")))\n",
    "    if sid and title:\n",
    "        service_map[sid] = title\n",
    "\n",
    "print(f\"   ‚úÖ Mapped {len(service_map)} services.\")\n",
    "\n",
    "# =====================================================\n",
    "# 3. Phase B: CSV Chunks (The Raw Info)\n",
    "# =====================================================\n",
    "print(\"üîπ [Phase B] Processing Raw CSV Chunks...\")\n",
    "chunk_files = sorted(glob.glob(os.path.join(Config.DATA_CHUNKS_DIR, \"*.csv\")))\n",
    "df_chunks = pd.concat([pd.read_csv(f) for f in chunk_files], ignore_index=True).fillna(\"\")\n",
    "\n",
    "csv_count = 0\n",
    "dropped_csv = 0\n",
    "\n",
    "for _, row in df_chunks.iterrows():\n",
    "    raw_text = str(row.get(\"chunk_text\", \"\")).strip()\n",
    "    clean_text = normalize_ar(raw_text)\n",
    "    \n",
    "    # Drop only noise (< 3 chars)\n",
    "    if len(clean_text) < 3: \n",
    "        dropped_csv += 1\n",
    "        continue\n",
    "        \n",
    "    # üíâ CONTEXT INJECTION\n",
    "    sid = str(row.get(\"service_id\", \"\")).strip()\n",
    "    # Use the Real Name from Phase A, fallback to chunk title\n",
    "    real_name = service_map.get(sid, normalize_ar(str(row.get(\"chunk_title\", \"\"))))\n",
    "    section_name = normalize_ar(str(row.get(\"chunk_title\", \"\")))\n",
    "    \n",
    "    # Format: \"Service: [Name] | Section: [Type] | Content: [Text]\"\n",
    "    enriched_text = f\"ÿßŸÑÿÆÿØŸÖÿ©: {real_name} | ÿßŸÑŸÇÿ≥ŸÖ: {section_name} | ÿßŸÑŸÖÿ≠ÿ™ŸàŸâ: {clean_text}\"\n",
    "    \n",
    "    meta = {\n",
    "        \"source\": \"csv\",\n",
    "        \"service_id\": sid,\n",
    "        \"type\": \"raw_info\"\n",
    "    }\n",
    "    processed_docs.append(Document(page_content=enriched_text, metadata=meta))\n",
    "    csv_count += 1\n",
    "\n",
    "print(f\"   ‚úÖ Processed {csv_count} chunks (Dropped {dropped_csv}).\")\n",
    "\n",
    "# =====================================================\n",
    "# 4. Phase C: SFT Data Injection (The Golden QA Pairs) üíé\n",
    "# =====================================================\n",
    "print(f\"üîπ [Phase C] Injecting SFT Q&A Pairs from: {os.path.basename(SFT_DATA_PATH)}\")\n",
    "sft_count = 0\n",
    "\n",
    "if os.path.exists(SFT_DATA_PATH):\n",
    "    with open(SFT_DATA_PATH, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                data = json.loads(line)\n",
    "                q = normalize_ar(data.get(\"instruction\", \"\"))\n",
    "                a = normalize_ar(data.get(\"response\", \"\"))\n",
    "                \n",
    "                if len(q) < 5 or len(a) < 5: continue\n",
    "                \n",
    "                # üíâ QA FORMAT INJECTION\n",
    "                # This helps the model find exact answers to similar questions\n",
    "                qa_text = f\"ÿ≥ÿ§ÿßŸÑ: {q} | ÿßŸÑÿ¨Ÿàÿßÿ® ÿßŸÑÿ±ÿ≥ŸÖŸä: {a}\"\n",
    "                \n",
    "                meta = {\n",
    "                    \"source\": \"sft_golden_data\",\n",
    "                    \"type\": \"qa_pair\"\n",
    "                }\n",
    "                processed_docs.append(Document(page_content=qa_text, metadata=meta))\n",
    "                sft_count += 1\n",
    "            except:\n",
    "                continue\n",
    "    print(f\"   ‚úÖ Injected {sft_count} Golden Q&A pairs.\")\n",
    "else:\n",
    "    print(f\"   ‚ö†Ô∏è WARNING: SFT file not found at path!\")\n",
    "\n",
    "# =====================================================\n",
    "# 5. Build & Save Vector DB\n",
    "# =====================================================\n",
    "total_docs = len(processed_docs)\n",
    "print(f\"üìä Total Knowledge Base Size: {total_docs} Documents.\")\n",
    "\n",
    "print(\"üîπ Initializing Embedding Model (BGE-M3)...\")\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=Config.EMBEDDING_MODEL_NAME,\n",
    "    model_kwargs={\"device\": \"cuda\"},\n",
    "    encode_kwargs={\"normalize_embeddings\": True}\n",
    ")\n",
    "\n",
    "print(\"‚ö° Building Ultimate Vector DB...\")\n",
    "vector_store = FAISS.from_documents(processed_docs, embedding=embedding_model)\n",
    "vector_store.save_local(Config.VECTOR_DB_DIR)\n",
    "print(f\"‚úÖ Saved to: {Config.VECTOR_DB_DIR}\")\n",
    "\n",
    "print(\"üéâ CELL 1 COMPLETE.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be16e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title üß† CELL 2 ‚Äî Load AI Engines (SFT + RAG + Whisper)\n",
    "# English-only comments as requested.\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "print(\"üîß Loading AI Engines (Golden Setup)...\")\n",
    "\n",
    "# =====================================================\n",
    "# 1. Load Embedding Model & Vector Store\n",
    "# =====================================================\n",
    "print(\"üîπ [1/4] Loading BGE-M3 Embeddings...\")\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=Config.EMBEDDING_MODEL_NAME,\n",
    "    model_kwargs={\"device\": \"cuda\"},\n",
    "    encode_kwargs={\"normalize_embeddings\": True}\n",
    ")\n",
    "\n",
    "print(f\"üîπ [2/4] Loading Vector Database from: {Config.VECTOR_DB_DIR}\")\n",
    "try:\n",
    "    vector_store = FAISS.load_local(\n",
    "        Config.VECTOR_DB_DIR, \n",
    "        embedding_model, \n",
    "        allow_dangerous_deserialization=True\n",
    "    )\n",
    "    print(\"‚úÖ Vector Store Loaded Successfully.\")\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"‚ùå Failed to load Vector DB! Run CELL 1 first. Error: {e}\")\n",
    "\n",
    "# =====================================================\n",
    "# 2. Load Whisper (ASR) Model (ADDED FOR AUDIO SUPPORT)\n",
    "# =====================================================\n",
    "print(f\"üîπ [3/4] Loading Whisper ASR ({Config.ASR_MODEL_NAME})...\")\n",
    "try:\n",
    "    asr_pipe = pipeline(\n",
    "        \"automatic-speech-recognition\",\n",
    "        model=Config.ASR_MODEL_NAME,\n",
    "        device=\"cuda:0\",\n",
    "        torch_dtype=torch.float16,\n",
    "        model_kwargs={\"attn_implementation\": \"sdpa\"} # Fast attention\n",
    "    )\n",
    "    print(\"‚úÖ Whisper Model Loaded.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Warning: Failed to load Whisper. Audio features will be disabled. Error: {e}\")\n",
    "    asr_pipe = None\n",
    "\n",
    "# =====================================================\n",
    "# 3. Load SFT Merged Model (The Brain)\n",
    "# =====================================================\n",
    "print(f\"üîπ [4/4] Loading SFT Model: {Config.LLM_MODEL_PATH.name}...\")\n",
    "\n",
    "# Check availability\n",
    "if not Config.LLM_MODEL_PATH.exists():\n",
    "    raise FileNotFoundError(f\"‚ùå SFT Model not found at: {Config.LLM_MODEL_PATH}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(Config.LLM_MODEL_PATH, use_fast=False)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    Config.LLM_MODEL_PATH,\n",
    "    torch_dtype=torch.bfloat16,  # Best for A100\n",
    "    device_map=\"auto\",\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "\n",
    "# =====================================================\n",
    "# 4. Create Specialized Pipelines\n",
    "# =====================================================\n",
    "\n",
    "# A) Chat Pipeline (Creative but Controlled)\n",
    "chat_pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=Config.GEN_MAX_TOKENS,\n",
    "    do_sample=True,\n",
    "    temperature=Config.GEN_TEMP,\n",
    "    top_p=0.9,\n",
    "    repetition_penalty=Config.GEN_REP_PENALTY,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "# B) Translation Pipeline (Strict/Greedy)\n",
    "trans_pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=512,\n",
    "    do_sample=False, # Greedy decoding for accuracy\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ All Engines Ready:\")\n",
    "print(\"   - Memory: Vector Store (BGE-M3)\")\n",
    "print(\"   - Ears:   Whisper ASR\")\n",
    "print(\"   - Brain:  SFT Model (Chat + Translation)\")\n",
    "print(\"üéâ CELL 2 COMPLETE.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f26eb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title üîó CELL 3 ‚Äî Hybrid RAG Chain (The Brain with Query Translation)\n",
    "# English-only comments as requested.\n",
    "\n",
    "import re\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "\n",
    "print(\"üß† Building the Logic Chain...\")\n",
    "\n",
    "# =====================================================\n",
    "# 1. Setup Retrievers (Hybrid)\n",
    "# =====================================================\n",
    "# Ensure processed_docs exists from Cell 1\n",
    "if 'processed_docs' not in globals():\n",
    "    raise RuntimeError(\"‚ùå 'processed_docs' missing. Please run CELL 1 first.\")\n",
    "\n",
    "# BM25 for keyword matching (Exact numbers, service names)\n",
    "bm25_retriever = BM25Retriever.from_documents(processed_docs)\n",
    "bm25_retriever.k = Config.RETRIEVAL_K\n",
    "\n",
    "# FAISS for semantic matching (Concepts)\n",
    "faiss_retriever = vector_store.as_retriever(search_kwargs={\"k\": Config.RETRIEVAL_K})\n",
    "\n",
    "# =====================================================\n",
    "# 2. Helper Functions (The Secret Sauce)\n",
    "# =====================================================\n",
    "def detect_language(text):\n",
    "    \"\"\"Simple heuristic: if it has English letters, treat as EN.\"\"\"\n",
    "    return \"en\" if re.search(r\"[a-zA-Z]\", text) else \"ar\"\n",
    "\n",
    "def translate_text(text, target_lang):\n",
    "    \"\"\"\n",
    "    Uses the SFT model (Greedy Mode) to translate text.\n",
    "    Crucial for: EN Query -> AR Search -> EN Answer.\n",
    "    \"\"\"\n",
    "    if target_lang == \"ar\":\n",
    "        prompt = f\"Translate the following English text to Arabic. Provide *only* the translated text.\\n\\nEnglish: {text}\\n\\nArabic:\"\n",
    "        split_token = \"Arabic:\"\n",
    "    else:\n",
    "        prompt = f\"Translate the following Arabic text to English. Provide *only* the translated text.\\n\\nArabic: {text}\\n\\nEnglish:\"\n",
    "        split_token = \"English:\"\n",
    "\n",
    "    try:\n",
    "        # Use the strict translation pipeline (trans_pipe from Cell 2)\n",
    "        raw = trans_pipe(prompt)[0]['generated_text']\n",
    "        out = raw.split(split_token)[-1].strip()\n",
    "        \n",
    "        # Safety check: if model repeats prompt or fails\n",
    "        if len(out) < 2 or out == text:\n",
    "            return text \n",
    "        return out\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Translation Failed: {e}\")\n",
    "        return text\n",
    "\n",
    "# =====================================================\n",
    "# 3. The Hybrid Chain Class\n",
    "# =====================================================\n",
    "class HybridChain:\n",
    "    def __init__(self):\n",
    "        # MUST match the SFT training format\n",
    "        self.system_template = \"\"\"<s>[INST] <<SYS>>\n",
    "You are an expert assistant for Absher and MOI services.\n",
    "- Answer in the SAME language as the user's question.\n",
    "- Use the provided [Context] to answer accurately.\n",
    "- If the info is missing in the context, say \"Information is not available in the documents.\"\n",
    "<</SYS>>\n",
    "\n",
    "[Context]\n",
    "{context}\n",
    "\n",
    "[User Question]\n",
    "{question} [/INST]\"\"\"\n",
    "\n",
    "    def retrieve_hybrid(self, query):\n",
    "        \"\"\"Combines BM25 and FAISS results with deduplication.\"\"\"\n",
    "        # Get docs from both sources\n",
    "        docs_bm25 = bm25_retriever.invoke(query)\n",
    "        docs_dense = faiss_retriever.invoke(query)\n",
    "        \n",
    "        # Merge and Deduplicate (Round Robin)\n",
    "        seen = set()\n",
    "        final_docs = []\n",
    "        \n",
    "        import itertools\n",
    "        for d in itertools.chain.from_iterable(itertools.zip_longest(docs_dense, docs_bm25)):\n",
    "            if d and d.page_content not in seen:\n",
    "                final_docs.append(d)\n",
    "                seen.add(d.page_content)\n",
    "        \n",
    "        # Return top K unique docs\n",
    "        return final_docs[:Config.RERANK_TOP_K]\n",
    "\n",
    "    def answer(self, user_query):\n",
    "        # 1. Detect Language\n",
    "        lang = detect_language(user_query)\n",
    "        search_query = user_query\n",
    "\n",
    "        # 2. Query Translation (The Fix for English Search) üîÑ\n",
    "        if lang == \"en\":\n",
    "            print(f\"üîÑ Auto-Translating Query: '{user_query}' -> Arabic...\")\n",
    "            translated_query = translate_text(user_query, \"ar\")\n",
    "            # Only use translation if it looks valid\n",
    "            if translated_query and translated_query != user_query:\n",
    "                search_query = translated_query\n",
    "            print(f\"   ‚Ü≥ Search Query: '{search_query}'\")\n",
    "\n",
    "        # 3. Retrieve (Always searches in Arabic now to find the data)\n",
    "        docs = self.retrieve_hybrid(normalize_ar(search_query))\n",
    "        \n",
    "        # 4. Build Context\n",
    "        if not docs:\n",
    "            return \"Information is not available in the documents. / ÿßŸÑŸÖÿπŸÑŸàŸÖÿ© ÿ∫Ÿäÿ± ŸÖÿ™ŸàŸÅÿ±ÿ©.\"\n",
    "        \n",
    "        context_text = \"\\n\\n\".join([f\"- {d.page_content}\" for d in docs])\n",
    "\n",
    "        # 5. Generate Answer (SFT Model)\n",
    "        full_prompt = self.system_template.format(context=context_text, question=user_query)\n",
    "        \n",
    "        response_raw = chat_pipe(full_prompt)[0]['generated_text']\n",
    "        response_clean = response_raw.split(\"[/INST]\")[-1].strip()\n",
    "\n",
    "        # 6. Safety Net: Post-hoc Translation üõ°Ô∏è\n",
    "        # If user asked in EN but bot answered in AR (Context Leakage)\n",
    "        if lang == \"en\" and detect_language(response_clean) == \"ar\":\n",
    "            print(\"‚ö†Ô∏è Context Leakage Detected (AR response to EN query). Auto-Fixing...\")\n",
    "            response_clean = translate_text(response_clean, \"en\")\n",
    "\n",
    "        return response_clean\n",
    "\n",
    "# Initialize\n",
    "hybrid_chain = HybridChain()\n",
    "print(\"‚úÖ Hybrid RAG Chain is Ready (With Auto-Translation Logic).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a7fd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title ü§ñ CELL 5 ‚Äî Pro Chat UI (MOI Edition with Language Selection) üá∏üá¶\n",
    "# Multi-stage UI: Language Selection -> Chat Interface\n",
    "\n",
    "import gradio as gr\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Disable tokenizers parallelism\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "print(\"üöÄ Launching MOI Branded Interface...\")\n",
    "\n",
    "# =====================================================\n",
    "# 1. Custom CSS\n",
    "# =====================================================\n",
    "moi_css = \"\"\"\n",
    ".moi-header {\n",
    "    text-align: center;\n",
    "    padding: 30px;\n",
    "    background: linear-gradient(90deg, #006C35 0%, #004D26 100%);\n",
    "    border-radius: 10px;\n",
    "    color: white;\n",
    "    margin-bottom: 20px;\n",
    "}\n",
    ".moi-header h1 { color: white !important; font-size: 2.5em; }\n",
    ".lang-btn { font-size: 1.2em; height: 60px; }\n",
    "\"\"\"\n",
    "\n",
    "# =====================================================\n",
    "# 2. Logic Handlers\n",
    "# =====================================================\n",
    "def start_chat(lang):\n",
    "    \"\"\"Transition from Welcome Screen to Chat Screen\"\"\"\n",
    "    # Set welcome message based on language\n",
    "    if lang == \"Arabic\":\n",
    "        greeting = [(None, \"üëã ÿ≠ŸäÿßŸÉ ÿßŸÑŸÑŸá! ÿ£ŸÜÿß ŸÖÿ≥ÿßÿπÿØŸÉ ÿßŸÑÿ∞ŸÉŸä ŸÑÿÆÿØŸÖÿßÿ™ Ÿàÿ≤ÿßÿ±ÿ© ÿßŸÑÿØÿßÿÆŸÑŸäÿ©. ÿ™ŸÅÿ∂ŸÑ ÿ®ÿ∑ÿ±ÿ≠ ÿ≥ÿ§ÿßŸÑŸÉ.\")]\n",
    "        rtl = True\n",
    "        label = \"ÿßŸÑŸÖÿ≠ÿßÿØÿ´ÿ© ÿßŸÑŸÅŸàÿ±Ÿäÿ©\"\n",
    "        placeholder = \"ÿßŸÉÿ™ÿ® ÿ≥ÿ§ÿßŸÑŸÉ ŸáŸÜÿß...\"\n",
    "    else:\n",
    "        greeting = [(None, \"üëã Hello! I am your MOI Smart Assistant. How can I help you today?\")]\n",
    "        rtl = False\n",
    "        label = \"Live Chat\"\n",
    "        placeholder = \"Type your question here...\"\n",
    "        \n",
    "    return (\n",
    "        gr.update(visible=False), # Hide Welcome\n",
    "        gr.update(visible=True),  # Show Chat\n",
    "        greeting,                 # Set History\n",
    "        lang,                     # Set State\n",
    "        gr.update(value=lang),    # Update Radio\n",
    "        gr.update(label=label, rtl=rtl), # Update Chatbot\n",
    "        gr.update(placeholder=placeholder, rtl=rtl) # Update Input\n",
    "    )\n",
    "\n",
    "def chat_response(message, history, audio_file, lang_val):\n",
    "    if audio_file:\n",
    "        try:\n",
    "            target_lang = \"ar\" if lang_val == \"Arabic\" else \"en\"\n",
    "            if 'asr_pipe' in globals() and asr_pipe:\n",
    "                text = asr_pipe(audio_file, generate_kwargs={\"language\": target_lang})[\"text\"].strip()\n",
    "                message = text\n",
    "                user_display = f\"üé§ {text}\"\n",
    "            else:\n",
    "                return history + [[None, \"‚ö†Ô∏è Whisper not loaded\"]]\n",
    "        except Exception as e:\n",
    "            return history + [[None, f\"‚ùå Error: {e}\"]]\n",
    "    else:\n",
    "        user_display = message\n",
    "\n",
    "    if not message: return history\n",
    "\n",
    "    try:\n",
    "        if 'hybrid_chain' in globals():\n",
    "            response = hybrid_chain.answer(message)\n",
    "        else:\n",
    "            response = \"‚ö†Ô∏è System Warning: Chain not loaded.\"\n",
    "    except Exception as e:\n",
    "        response = f\"‚ùå Error: {e}\"\n",
    "\n",
    "    history.append((user_display, response))\n",
    "    return history\n",
    "\n",
    "def reset_app():\n",
    "    \"\"\"Return to language selection\"\"\"\n",
    "    return (\n",
    "        gr.update(visible=True),  # Show Welcome\n",
    "        gr.update(visible=False), # Hide Chat\n",
    "        None,                     # Clear State\n",
    "        []                        # Clear History\n",
    "    )\n",
    "\n",
    "def clear_inputs(): return \"\", None\n",
    "\n",
    "# =====================================================\n",
    "# 3. Layout\n",
    "# =====================================================\n",
    "with gr.Blocks(theme=gr.themes.Glass(), css=moi_css, title=\"MOI Assistant\") as demo:\n",
    "    \n",
    "    # State to hold language preference\n",
    "    lang_state = gr.State(value=\"Arabic\")\n",
    "\n",
    "    # --- Header ---\n",
    "    gr.HTML(\"\"\"\n",
    "    <div class='moi-header'>\n",
    "        <h1>ÿßŸÑŸÖÿ≥ÿßÿπÿØ ÿßŸÑÿ∞ŸÉŸä ŸÑŸàÿ≤ÿßÿ±ÿ© ÿßŸÑÿØÿßÿÆŸÑŸäÿ©</h1>\n",
    "        <p>MOI Smart Assistant | Powered by ALLaM-7B</p>\n",
    "    </div>\n",
    "    \"\"\")\n",
    "    \n",
    "    # --- SCREEN 1: Welcome & Language Selection ---\n",
    "    with gr.Group(visible=True) as welcome_screen:\n",
    "        gr.Markdown(\"### üåç Please select your preferred language / ÿßŸÑÿ±ÿ¨ÿßÿ° ÿßÿÆÿ™Ÿäÿßÿ± ÿßŸÑŸÑÿ∫ÿ© ÿßŸÑŸÖŸÅÿ∂ŸÑÿ©\", elem_id=\"lang-text\")\n",
    "        with gr.Row():\n",
    "            btn_ar = gr.Button(\"ÿßŸÑÿπÿ±ÿ®Ÿäÿ© üá∏üá¶\", variant=\"primary\", elem_classes=[\"lang-btn\"])\n",
    "            btn_en = gr.Button(\"English üá¨üáß\", variant=\"secondary\", elem_classes=[\"lang-btn\"])\n",
    "\n",
    "    # --- SCREEN 2: Chat Interface ---\n",
    "    with gr.Group(visible=False) as chat_screen:\n",
    "        with gr.Row():\n",
    "            # Left: Chat\n",
    "            with gr.Column(scale=3):\n",
    "                chatbot = gr.Chatbot(label=\"Chat\", height=500)\n",
    "                with gr.Row():\n",
    "                    msg = gr.Textbox(show_label=False, container=False, scale=4)\n",
    "                    submit_btn = gr.Button(\"üöÄ\", variant=\"primary\", scale=1)\n",
    "\n",
    "            # Right: Settings\n",
    "            with gr.Column(scale=1):\n",
    "                gr.Markdown(\"### ‚öôÔ∏è Settings\")\n",
    "                audio_input = gr.Audio(source=\"microphone\", type=\"filepath\", label=\"Voice Input\")\n",
    "                # Hidden radio just to store state visually if needed\n",
    "                lang_display = gr.Radio([\"Arabic\", \"English\"], label=\"Language\", interactive=False)\n",
    "                \n",
    "                gr.Markdown(\"---\")\n",
    "                restart_btn = gr.Button(\"üîÑ Change Language / ÿ™ÿ∫ŸäŸäÿ± ÿßŸÑŸÑÿ∫ÿ©\", variant=\"secondary\")\n",
    "\n",
    "    # =====================================================\n",
    "    # 4. Events\n",
    "    # =====================================================\n",
    "    \n",
    "    # Language Selection\n",
    "    btn_ar.click(\n",
    "        fn=lambda: start_chat(\"Arabic\"),\n",
    "        outputs=[welcome_screen, chat_screen, chatbot, lang_state, lang_display, chatbot, msg]\n",
    "    )\n",
    "    btn_en.click(\n",
    "        fn=lambda: start_chat(\"English\"),\n",
    "        outputs=[welcome_screen, chat_screen, chatbot, lang_state, lang_display, chatbot, msg]\n",
    "    )\n",
    "\n",
    "    # Chat Actions\n",
    "    msg.submit(chat_response, [msg, chatbot, audio_input, lang_state], [chatbot]) \\\n",
    "       .then(clear_inputs, None, [msg, audio_input])\n",
    "    \n",
    "    submit_btn.click(chat_response, [msg, chatbot, audio_input, lang_state], [chatbot]) \\\n",
    "              .then(clear_inputs, None, [msg, audio_input])\n",
    "\n",
    "    # Restart\n",
    "    restart_btn.click(reset_app, outputs=[welcome_screen, chat_screen, lang_state, chatbot])\n",
    "\n",
    "# =====================================================\n",
    "# 5. Launch\n",
    "# =====================================================\n",
    "print(\"‚úÖ Launching MOI App...\")\n",
    "demo.queue().launch(share=True, inline=False, show_api=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6f428b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
