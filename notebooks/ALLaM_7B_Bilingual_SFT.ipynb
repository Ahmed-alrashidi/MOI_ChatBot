{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04aa4834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Paths ready.\n",
      "Project Directory: /ibex/user/rashidah/projects/MOI_ChatBot/al-lam_bilingual_sft\n"
     ]
    }
   ],
   "source": [
    "# @title üöÄ CELL 1 ‚Äî Project Setup & Paths\n",
    "# ALLaM-7B ‚Äî Bilingual SFT for MOI RAG (Saudi Tone + English Answers When Asked)\n",
    "# Requirements: transformers>=4.44, peft>=0.12, accelerate>=0.34, bitsandbytes, trl>=0.9\n",
    "\n",
    "import os, sys, json, math, random, shutil, logging, warnings\n",
    "from pathlib import Path\n",
    "\n",
    "# Base model (your local path)\n",
    "BASE_MODEL_PATH = \"/ibex/user/rashidah/projects/MOI_ChatBot/chatbot_project/3_models/models--ALLaM-AI--ALLaM-7B-Instruct-preview\"\n",
    "\n",
    "# Project dirs\n",
    "PROJ_DIR = Path(\"/ibex/user/rashidah/projects/MOI_ChatBot/al-lam_bilingual_sft\")\n",
    "DATA_DIR = PROJ_DIR / \"data\"\n",
    "CKPT_DIR = PROJ_DIR / \"checkpoints\"\n",
    "OUT_DIR  = PROJ_DIR / \"artifacts\"\n",
    "for p in [DATA_DIR, CKPT_DIR, OUT_DIR]: p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "print(\"‚úÖ Paths ready.\")\n",
    "print(f\"Project Directory: {PROJ_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88be0d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Master rows loaded: 64\n",
      "‚úÖ Chunk rows loaded: 256\n"
     ]
    }
   ],
   "source": [
    "# @title üì¶ CELL 2 ‚Äî Load Raw CSV Data\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os # Ensure os is imported if running standalone\n",
    "\n",
    "# Use the same sources you already validated\n",
    "MASTER_DIR = \"/ibex/user/rashidah/projects/MOI_ChatBot/chatbot_project/1_data/Data_Master\"\n",
    "CHUNKS_DIR = \"/ibex/user/rashidah/projects/MOI_ChatBot/chatbot_project/1_data/Data_chunks\"\n",
    "\n",
    "def load_csvs(d):\n",
    "    return [pd.read_csv(p) for p in sorted(glob.glob(os.path.join(d, \"*.csv\")))]\n",
    "\n",
    "df_master = pd.concat(load_csvs(MASTER_DIR), ignore_index=True)\n",
    "df_chunks = pd.concat(load_csvs(CHUNKS_DIR), ignore_index=True)\n",
    "\n",
    "# Keep Arabic text fields used in RAG answers\n",
    "df_master = df_master.fillna(\"\")\n",
    "df_chunks = df_chunks.fillna(\"\")\n",
    "\n",
    "print(f\"‚úÖ Master rows loaded: {len(df_master)}\")\n",
    "print(f\"‚úÖ Chunk rows loaded: {len(df_chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f90fbf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading raw CSV data...\n",
      "Loaded 64 master rows and 256 chunk rows.\n",
      "Loading ALLaM-7B (4-bit) to use as a translator...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "920aaa8c4a264e6a983d134c8818b6b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting pad_token_id = eos_token_id for batching.\n",
      "‚úÖ ALLaM-7B translator loaded on GPU.\n",
      "Pass 1: Collecting texts for translation...\n",
      "Pass 2: Translating 170 unique text snippets...\n",
      "Translating 170 texts with ALLaM...\n",
      "‚úÖ Translation complete.\n",
      "Pass 3: Building final bilingual dataset...\n",
      "\n",
      "‚úÖ‚úÖ‚úÖ New SFT records generated: 438\n",
      "‚úÖ New Dataset saved to: /ibex/user/rashidah/projects/MOI_ChatBot/al-lam_bilingual_sft/data/bilingual_moi_absher_sFT_v2.jsonl\n",
      "\n",
      "üí° NOTE: CELL 5 is now ready to use this new train_path.\n"
     ]
    }
   ],
   "source": [
    "# @title üìä CELL 3 ‚Äî Bilingual Dataset Generation (PADDING_SIDE FIX)\n",
    "# %%\n",
    "import re, unicodedata, random, os\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM, \n",
    "    BitsAndBytesConfig, \n",
    "    pipeline\n",
    ")\n",
    "import glob # Make sure glob is imported\n",
    "\n",
    "# --- 1. Load Data (Same as before) ---\n",
    "print(\"Loading raw CSV data...\")\n",
    "# Ensure df_master and df_chunks are loaded from CELL 2\n",
    "if 'df_master' not in globals() or df_master.empty:\n",
    "    MASTER_DIR = \"/ibex/user/rashidah/projects/MOI_ChatBot/chatbot_project/1_data/Data_Master\"\n",
    "    df_master = pd.concat([pd.read_csv(p) for p in sorted(glob.glob(os.path.join(MASTER_DIR, \"*.csv\")))], ignore_index=True)\n",
    "    df_master = df_master.fillna(\"\")\n",
    "\n",
    "if 'df_chunks' not in globals() or df_chunks.empty:\n",
    "    CHUNKS_DIR = \"/ibex/user/rashidah/projects/MOI_ChatBot/chatbot_project/1_data/Data_chunks\"\n",
    "    df_chunks = pd.concat([pd.read_csv(p) for p in sorted(glob.glob(os.path.join(CHUNKS_DIR, \"*.csv\")))], ignore_index=True)\n",
    "    df_chunks = df_chunks.fillna(\"\")\n",
    "\n",
    "print(f\"Loaded {len(df_master)} master rows and {len(df_chunks)} chunk rows.\")\n",
    "\n",
    "# --- 2. Arabic Normalization (Same as before) ---\n",
    "def normalize_ar(s):\n",
    "    if not isinstance(s, str): return \"\"\n",
    "    s = unicodedata.normalize(\"NFC\", s)\n",
    "    s = re.sub(r\"[\\u0617-\\u061A\\u064B-\\u0652\\u0670\\u06D6-\\u06ED]\", \"\", s)\n",
    "    s = re.sub(r\"[ÿ£ÿ•ÿ¢Ÿ±]\", \"ÿß\", s).replace(\"Ÿâ\", \"Ÿä\")\n",
    "    return re.sub(r\"\\s+\", \" \", s).strip()\n",
    "\n",
    "# --- 3. Templates (Same as before) ---\n",
    "EN_Q_TEMPLATES = [\n",
    "    \"How can I {x}?\", \"What are the steps to {x}?\",\n",
    "    \"Where is the service for {x} in Absher?\", \"Requirements/fees for {x}?\"\n",
    "]\n",
    "AR_Q_TEMPLATES = [\n",
    "    \"ŸÉŸäŸÅ ÿ£ŸÇÿØÿ± {x}ÿü\", \"ŸÖÿß ŸáŸä ÿÆÿ∑Ÿàÿßÿ™ {x}ÿü\",\n",
    "    \"ŸàŸäŸÜ ÿ£ŸÑŸÇŸâ ÿÆÿØŸÖÿ© {x} ŸÅŸä ÿ£ÿ®ÿ¥ÿ±ÿü\", \"Ÿàÿ¥ ÿßŸÑÿ¥ÿ±Ÿàÿ∑/ÿßŸÑÿ±ÿ≥ŸàŸÖ ŸÑŸÄ {x}ÿü\"\n",
    "]\n",
    "\n",
    "def ar_action_from_row(r):\n",
    "    title = normalize_ar(str(r.get(\"service_title_ar\",\"\")))\n",
    "    return title if len(title) >= 6 else normalize_ar(str(r.get(\"description_full\",\"\"))[:80])\n",
    "\n",
    "def build_ar_answer_text(r):\n",
    "    parts = []\n",
    "    desc = normalize_ar(r.get(\"description_full\", \"\"))\n",
    "    cond = normalize_ar(r.get(\"conditions\", \"\"))\n",
    "    fees = normalize_ar(r.get(\"fees\", \"\"))\n",
    "    path = normalize_ar(r.get(\"access_path\", \"\"))\n",
    "    if desc: parts.append(f\"ÿßŸÑŸàÿµŸÅ: {desc}\")\n",
    "    if cond: parts.append(f\"ÿßŸÑÿ¥ÿ±Ÿàÿ∑: {cond}\")\n",
    "    if fees: parts.append(f\"ÿßŸÑÿ±ÿ≥ŸàŸÖ: {fees}\")\n",
    "    if path: parts.append(f\"ÿ∑ÿ±ŸäŸÇÿ© ÿßŸÑŸàÿµŸàŸÑ: {path}\")\n",
    "    return \" | \".join(parts)\n",
    "\n",
    "# --- 4. üî¥ NEW: ALLaM-7B Translation Pipeline ---\n",
    "def load_allam_translator():\n",
    "    \"\"\"Loads the 4-bit ALLaM model, identical to CELL 5.\"\"\"\n",
    "    print(\"Loading ALLaM-7B (4-bit) to use as a translator...\")\n",
    "    \n",
    "    # Path to the *working* model snapshot\n",
    "    model_path = \"/ibex/user/rashidah/projects/MOI_ChatBot/chatbot_project/3_models/models--ALLaM-AI--ALLaM-7B-Instruct-preview/snapshots/a28dd1e67420cde72d3629c8633a974cf7d9c366\"\n",
    "    \n",
    "    bnb_cfg = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "    )\n",
    "    \n",
    "    # --- üî¥ PADDING_SIDE FIX (As suggested by the warning) üî¥ ---\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_path, \n",
    "        use_fast=False,\n",
    "        padding_side='left' # <-- This is the fix\n",
    "    )\n",
    "    # --- End of Fix ---\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_path, \n",
    "        quantization_config=bnb_cfg,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    \n",
    "    # --- BATCHING FIX (As suggested by the *previous* error) ---\n",
    "    if tokenizer.pad_token_id is None:\n",
    "        tokenizer.pad_token_id = model.config.eos_token_id\n",
    "        print(\"Setting pad_token_id = eos_token_id for batching.\")\n",
    "    # --- End of Fix ---\n",
    "\n",
    "    translator_pipe = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        dtype=torch.bfloat16, \n",
    "        max_new_tokens=512, \n",
    "        do_sample=False,    \n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    print(\"‚úÖ ALLaM-7B translator loaded on GPU.\")\n",
    "    return translator_pipe\n",
    "\n",
    "def translate_batch_with_allam(translator, text_list_ar):\n",
    "    \"\"\"Translates a list of Arabic texts using the ALLaM pipeline.\"\"\"\n",
    "    \n",
    "    prompts = []\n",
    "    for text in text_list_ar:\n",
    "        prompt = f\"Translate the following Arabic text to English. Provide *only* the translated English text.\\n\\nArabic: {text}\\n\\nEnglish:\"\n",
    "        prompts.append(prompt)\n",
    "    \n",
    "    print(f\"Translating {len(prompts)} texts with ALLaM...\")\n",
    "    \n",
    "    # Use batching (batch_size=8 is safe for A100 80GB)\n",
    "    results_raw = translator(prompts, batch_size=8)\n",
    "    \n",
    "    cleaned_results = []\n",
    "    for result in results_raw:\n",
    "        generated_text = result[0]['generated_text']\n",
    "        translated = generated_text.split(\"English:\")[-1].strip()\n",
    "        cleaned_results.append(translated)\n",
    "        \n",
    "    return cleaned_results\n",
    "\n",
    "# --- 5. Generate Dataset ---\n",
    "\n",
    "def generate_sft_data(translator):\n",
    "    if translator is None:\n",
    "        raise RuntimeError(\"Translation model failed to load. Cannot proceed.\")\n",
    "\n",
    "    records = []\n",
    "    texts_to_translate = set()\n",
    "    \n",
    "    print(\"Pass 1: Collecting texts for translation...\")\n",
    "    master_answers = {}\n",
    "    for _, r in df_master.iterrows() :\n",
    "        ans_ar = build_ar_answer_text(r)\n",
    "        if not ans_ar: continue\n",
    "        master_answers[r['service_id']] = ans_ar\n",
    "        texts_to_translate.add(ans_ar)\n",
    "\n",
    "    chunk_texts = {}\n",
    "    for _, r in df_chunks.iterrows():\n",
    "        txt = normalize_ar(str(r.get(\"chunk_text\",\"\")))\n",
    "        if len(txt) < 40: continue\n",
    "        chunk_texts[r['chunk_id']] = txt\n",
    "        texts_to_translate.add(txt)\n",
    "\n",
    "    print(f\"Pass 2: Translating {len(texts_to_translate)} unique text snippets...\")\n",
    "    text_list_ar = list(texts_to_translate)\n",
    "    \n",
    "    translated_texts = translate_batch_with_allam(translator, text_list_ar)\n",
    "    \n",
    "    translation_map = {\n",
    "        ar_text: en_text\n",
    "        for ar_text, en_text in zip(text_list_ar, translated_texts)\n",
    "    }\n",
    "    print(\"‚úÖ Translation complete.\")\n",
    "\n",
    "    print(\"Pass 3: Building final bilingual dataset...\")\n",
    "    random.seed(1337)\n",
    "\n",
    "    # Master-level pairs\n",
    "    for _, r in df_master.iterrows():\n",
    "        sid = r['service_id']\n",
    "        if sid not in master_answers: continue\n",
    "        action = ar_action_from_row(r)\n",
    "        ans_ar = master_answers[sid]\n",
    "        ans_en = translation_map.get(ans_ar)\n",
    "        if not action or not ans_en: continue\n",
    "\n",
    "        q_ar = random.choice(AR_Q_TEMPLATES).format(x=action)\n",
    "        a_ar = f\"ÿ£ŸÉŸäÿØ. ÿ®ŸÜÿßÿ°Ÿã ÿπŸÑŸâ ŸÇÿßÿπÿØÿ© ÿßŸÑŸÖÿπÿ±ŸÅÿ©: {ans_ar}\"\n",
    "        records.append({\"instruction\": q_ar, \"response\": a_ar})\n",
    "        \n",
    "        q_en = random.choice(EN_Q_TEMPLATES).format(x=action)\n",
    "        a_en = f\"Certainly. Based on the knowledge base: {ans_en}\"\n",
    "        records.append({\"instruction\": q_en, \"response\": a_en})\n",
    "\n",
    "    # Chunk-level pairs\n",
    "    for _, r in df_chunks.iterrows():\n",
    "        cid = r['chunk_id']\n",
    "        if cid not in chunk_texts: continue\n",
    "        title = normalize_ar(str(r.get(\"chunk_title\",\"\") or \"the service\"))\n",
    "        txt_ar = chunk_texts[cid]\n",
    "        txt_en = translation_map.get(txt_ar)\n",
    "        if not txt_en: continue\n",
    "\n",
    "        q_ar = f\"ÿπÿ∑ŸÜŸä ŸÖŸÑÿÆÿµ ŸÖŸàÿ¨ÿ≤ ÿπŸÜ: {title}\"\n",
    "        a_ar = f\"ÿ®ŸÜÿßÿ°Ÿã ÿπŸÑŸâ ÿßŸÑŸÖÿ≥ÿ™ŸÜÿØÿßÿ™: {txt_ar[:450]}\"\n",
    "        records.append({\"instruction\": q_ar, \"response\": a_ar})\n",
    "        \n",
    "        q_en = f\"Give a concise English summary about: {title}\"\n",
    "        a_en = f\"Based on the documents: {txt_en[:450]}\"\n",
    "        records.append({\"instruction\": q_en, \"response\": a_en})\n",
    "\n",
    "    return records\n",
    "\n",
    "# --- 6. Execute and Save ---\n",
    "translator_model_pipe = None\n",
    "# Define the new train_path which will be used by CELL 5\n",
    "train_path = DATA_DIR / \"bilingual_moi_absher_sFT_v2.jsonl\"\n",
    "\n",
    "try:\n",
    "    torch.cuda.empty_cache() \n",
    "    translator_model_pipe = load_allam_translator()\n",
    "    sft_records = generate_sft_data(translator_model_pipe)\n",
    "    \n",
    "    with open(train_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for ex in sft_records:\n",
    "            f.write(json.dumps(ex, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    print(f\"\\n‚úÖ‚úÖ‚úÖ New SFT records generated: {len(sft_records)}\")\n",
    "    print(f\"‚úÖ New Dataset saved to: {train_path}\")\n",
    "    print(\"\\nüí° NOTE: CELL 5 is now ready to use this new train_path.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå An error occurred during dataset generation: {e}\")\n",
    "finally:\n",
    "    if translator_model_pipe:\n",
    "        del translator_model_pipe\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af8fd668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ System Rules defined.\n"
     ]
    }
   ],
   "source": [
    "# @title üìú CELL 4 ‚Äî System Prompt (Unchanged)\n",
    "# This prompt is excellent. It explicitly states the bilingual policy.\n",
    "\n",
    "SYSTEM_RULES = \"\"\"You are Absher Assistant. Policies:\n",
    "- Answer in the user's question language: Arabic‚ÜíArabic, English‚ÜíEnglish.\n",
    "- Saudi polite tone (friendly, concise).\n",
    "- Prefer factual content grounded in MOI/Absher knowledge.\n",
    "- If asked for general Saudi information (distance, regions), answer simply.\n",
    "- If info not in knowledge, say: \"ÿßŸÑŸÖÿπŸÑŸàŸÖÿ© ÿ∫Ÿäÿ± ŸÖÿ™ŸàŸÅÿ±ÿ© ŸÅŸä ÿßŸÑŸÖÿ≥ÿ™ŸÜÿØ.\" / \"Information is not available in the documents.\"\n",
    "- NEVER confuse National ID (Saudi citizen) with Iqama (resident ID).\"\"\"\n",
    "\n",
    "print(\"‚úÖ System Rules defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "977a395e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2525205/2066014571.py:11: FutureWarning: Support for Python 3.9 will be dropped in the next release (after its end-of-life on October 31, 2025). Please upgrade to Python 3.10 or newer.\n",
      "  from trl import SFTTrainer, SFTConfig\n",
      "/ibex/user/rashidah/software/miniconda3/envs/pytorch_env/lib/python3.9/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/ibex/user/rashidah/software/miniconda3/envs/pytorch_env/lib/python3.9/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up Tokenizer, Model (4-bit), and LoRA Config...\n",
      "Setting pad_token_id to eos_token_id\n",
      "Loading 4-bit base model (ALLaM-7B-Instruct)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47b5a97f9fb2497ebe964fb998ce21c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying PEFT (LoRA) to model...\n",
      "Loading dataset from: /ibex/user/rashidah/projects/MOI_ChatBot/al-lam_bilingual_sft/data/bilingual_moi_absher_sFT_v2.jsonl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20e97ca322524cbfa7c04fbc8e1a2192",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatting dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb65650a6a8b4419afa4b0fbf7edbcc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/438 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manually tokenizing dataset (max_length=4096)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4616f30ef0074291a5d318bfae089f1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/438 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a8fc49764f0411a94ac7bae07313bf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/438 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Trainer ready.\n"
     ]
    }
   ],
   "source": [
    "# @title ‚öôÔ∏è CELL 5 ‚Äî SFT Trainer Setup (ALL FIXES APPLIED)\n",
    "# %%\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "print(\"Setting up Tokenizer, Model (4-bit), and LoRA Config...\")\n",
    "\n",
    "# --- 1. Path Fix: Point to the correct snapshot directory ---\n",
    "BASE_MODEL_PATH = \"/ibex/user/rashidah/projects/MOI_ChatBot/chatbot_project/3_models/models--ALLaM-AI--ALLaM-7B-Instruct-preview/snapshots/a28dd1e67420cde72d3629c8633a974cf7d9c366\"\n",
    "\n",
    "# --- Tokenizer ---\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_PATH, use_fast=False)\n",
    "\n",
    "if tokenizer.pad_token_id is None:\n",
    "    print(\"Setting pad_token_id to eos_token_id\")\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# --- 4-bit Quantization (BNB) ---\n",
    "bnb_cfg = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16, # Use bfloat16 for A100\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# --- Load Base Model ---\n",
    "print(\"Loading 4-bit base model (ALLaM-7B-Instruct)...\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL_PATH, \n",
    "    quantization_config=bnb_cfg,\n",
    "    device_map=\"auto\" \n",
    ")\n",
    "\n",
    "# --- LoRA Config (Syntax Fix) ---\n",
    "lora_cfg = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05, # <-- Corrected 'a'\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"up_proj\", \"down_proj\", \"gate_proj\"\n",
    "    ]\n",
    ")\n",
    "print(\"Applying PEFT (LoRA) to model...\")\n",
    "model = get_peft_model(base_model, lora_cfg)\n",
    "\n",
    "# --- Dataset Loading and Formatting ---\n",
    "# üî¥ UPDATED PATH: Point to the new dataset generated by CELL 3\n",
    "train_path = DATA_DIR / \"bilingual_moi_absher_sFT_v2.jsonl\"\n",
    "print(f\"Loading dataset from: {train_path}\")\n",
    "# --- End of Update ---\n",
    "\n",
    "ds = load_dataset(\"json\", data_files=str(train_path))\n",
    "\n",
    "def format_example(ex):\n",
    "    prompt = f\"<s>[INST] <<SYS>>\\n{SYSTEM_RULES}\\n<</SYS>>\\n{ex['instruction']} [/INST]\"\n",
    "    return {\"text\": f\"{prompt} {ex['response']}\"}\n",
    "\n",
    "print(\"Formatting dataset...\")\n",
    "ds = ds.map(format_example, remove_columns=ds[\"train\"].column_names)\n",
    "\n",
    "\n",
    "# --- TRL Version Fix 1: Manual Tokenization ---\n",
    "MAX_SEQ_LENGTH = 4096 \n",
    "def tokenize_data(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"], \n",
    "        truncation=True, \n",
    "        padding=False, \n",
    "        max_length=MAX_SEQ_LENGTH\n",
    "    )\n",
    "\n",
    "print(f\"Manually tokenizing dataset (max_length={MAX_SEQ_LENGTH})...\")\n",
    "ds = ds.map(tokenize_data, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "# --- TRL Version Fix 2 & W&B Fix ---\n",
    "train_cfg = SFTConfig(\n",
    "    output_dir=str(CKPT_DIR),\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=8,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    learning_rate=2e-4,\n",
    "    logging_steps=25,\n",
    "    save_strategy=\"epoch\",\n",
    "    report_to=\"none\", # <-- W&B Fix\n",
    "    # packing=True, # <-- Removed for old TRL version\n",
    "    \n",
    "    # A100-specific optimizations\n",
    "    bf16=True,\n",
    "    tf32=True,\n",
    ")\n",
    "\n",
    "# --- TRL Version Fix 3: Remove unsupported args ---\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=ds[\"train\"],\n",
    "    args=train_cfg,\n",
    "    # tokenizer, dataset_text_field, max_seq_length REMOVED\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Trainer ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "216aeed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting SFT training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='14' max='14' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [14/14 00:57, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Training complete. LoRA adapter saved to: /ibex/user/rashidah/projects/MOI_ChatBot/al-lam_bilingual_sft/checkpoints/lora_adapter\n"
     ]
    }
   ],
   "source": [
    "# @title üöÄ CELL 6 ‚Äî Start Training\n",
    "# This cell is correct.\n",
    "\n",
    "print(\"Starting SFT training...\")\n",
    "trainer.train()\n",
    "\n",
    "# Save the adapter and tokenizer\n",
    "adapter_path = CKPT_DIR / \"lora_adapter\"\n",
    "trainer.save_model(str(adapter_path))\n",
    "tokenizer.save_pretrained(str(adapter_path))\n",
    "\n",
    "print(f\"‚úÖ Training complete. LoRA adapter saved to: {adapter_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a22a77a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging LoRA adapter into base model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7256ae925912437493e2da7facebfca5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing merge and unload...\n",
      "‚úÖ Merged standalone model saved to: /ibex/user/rashidah/projects/MOI_ChatBot/al-lam_bilingual_sft/artifacts/ALLaM-7B-MOI-Bilingual-Merged\n"
     ]
    }
   ],
   "source": [
    "# @title üíø CELL 7 ‚Äî Merge Adapter (dtype fix)\n",
    "# This cell merges the LoRA adapter with the base model.\n",
    "# %%\n",
    "from peft import PeftModel\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "print(\"Merging LoRA adapter into base model...\")\n",
    "\n",
    "merged_dir = OUT_DIR / \"ALLaM-7B-MOI-Bilingual-Merged\"\n",
    "\n",
    "# Define adapter path from previous cell\n",
    "adapter_path = CKPT_DIR / \"lora_adapter\"\n",
    "\n",
    "# Load base model in bf16 for merging\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL_PATH, \n",
    "    dtype=torch.bfloat16, # <-- Fixed 'torch_dtype' to 'dtype'\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Load the PEFT model\n",
    "model = PeftModel.from_pretrained(model, str(adapter_path))\n",
    "\n",
    "# Merge and unload\n",
    "print(\"Performing merge and unload...\")\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "# Save the merged model\n",
    "model.save_pretrained(str(merged_dir))\n",
    "tokenizer.save_pretrained(str(merged_dir))\n",
    "\n",
    "print(f\"‚úÖ Merged standalone model saved to: {merged_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44073ddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading merged model for testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test pipeline...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77bd38d30bdc45f6999b7795b9b2c8f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- TEST: AR‚ÜíAR ----\n",
      "ü§ñ Response: ÿ®ŸÜÿßÿ°Ÿã ÿπŸÑŸâ ÿßŸÑŸÖÿ≥ÿ™ŸÜÿØÿßÿ™: ÿßŸÑÿÆÿØŸÖÿßÿ™ > ÿÆÿØŸÖÿßÿ™ > ÿßŸÑÿ¨Ÿàÿßÿ≤ÿßÿ™ > ÿ™ÿ¨ÿØŸäÿØ ÿ¨Ÿàÿßÿ≤ ÿßŸÑÿ≥ŸÅÿ±. ÿßÿØÿÆŸÑ ÿπŸÑŸâ ÿÆÿØŸÖÿßÿ™ ÿßŸÑÿ¨Ÿàÿßÿ≤ÿßÿ™ > ÿ™ÿ¨ÿØŸäÿØ ÿ¨Ÿàÿßÿ≤ ÿßŸÑÿ≥ŸÅÿ±. ÿßŸÑÿ¥ÿ±Ÿàÿ∑: ÿ£ŸÜ ŸäŸÉŸàŸÜ ÿßŸÑŸÖÿ≥ÿ™ŸÅŸäÿØ ÿπŸÑŸâ ŸÇŸäÿØ ÿßŸÑÿ≠Ÿäÿßÿ©. ÿ£ŸÜ ŸÑÿß ŸäŸÇŸÑ ÿπŸÖÿ± ÿßŸÑŸÖÿ≥ÿ™ŸÅŸäÿØ ÿπŸÜ 21 ÿ≥ŸÜÿ©. ÿ£ŸÜ ŸÑÿß ŸäŸÉŸàŸÜ ŸáŸÜÿßŸÉ ÿ£Ÿä ŸÖŸÑÿßÿ≠ÿ∏ÿßÿ™ ÿ£ŸÖŸÜŸäÿ© ÿπŸÑŸâ ÿßŸÑŸÖÿ≥ÿ™ŸÅŸäÿØ. ÿ£ŸÜ ŸÑÿß ŸäŸÉŸàŸÜ ŸáŸÜÿßŸÉ ÿ£Ÿä ŸÖÿÆÿßŸÑŸÅÿßÿ™ ÿπŸÑŸâ ÿßŸÑŸÖÿ≥ÿ™ŸÅŸäÿØ. ÿ£ŸÜ ŸÑÿß ŸäŸÉŸàŸÜ ŸáŸÜÿßŸÉ ÿ£Ÿä ŸÖŸÜÿπ ÿ≥ŸÅÿ± ÿπŸÑŸâ ÿßŸÑŸÖÿ≥ÿ™ŸÅŸäÿØ. ÿ£ŸÜ ŸÑÿß ŸäŸÉŸàŸÜ ŸáŸÜÿßŸÉ ÿ£Ÿä ŸÖŸÜÿπ ÿ≥ŸÅÿ± ÿπŸÑŸâ ÿ£ÿ≠ÿØ ÿ£ŸÅÿ±ÿßÿØ ÿßŸÑÿ£ÿ≥ÿ±ÿ©. ÿ£ŸÜ ŸÑÿß ŸäŸÉŸàŸÜ ŸáŸÜÿßŸÉ ÿ£Ÿä ŸÖŸÜÿπ ÿÆÿ±Ÿàÿ¨ ŸàÿπŸàÿØÿ© ÿπŸÑŸâ ÿßŸÑŸÖÿ≥ÿ™ŸÅŸäÿØ. ÿ£ŸÜ ŸÑÿß ŸäŸÉŸàŸÜ ŸáŸÜÿßŸÉ ÿ£Ÿä ŸÖŸÜÿπ ÿÆÿ±Ÿàÿ¨ ŸÜŸáÿßÿ¶Ÿä ÿπŸÑŸâ ÿßŸÑŸÖÿ≥ÿ™ŸÅŸäÿØ. ÿ£ŸÜ ŸÑÿß ŸäŸÉŸàŸÜ ŸáŸÜÿßŸÉ ÿ£Ÿä ŸÖŸÜÿπ ÿÆÿ±Ÿàÿ¨ ŸÜŸáÿßÿ¶Ÿä ÿπŸÑŸâ ÿ£ÿ≠ÿØ ÿ£ŸÅÿ±ÿßÿØ ÿßŸÑÿ£ÿ≥ÿ±ÿ©. ÿ£ŸÜ ŸÑÿß ŸäŸÉŸàŸÜ ŸáŸÜÿßŸÉ ÿ£Ÿä ŸÖŸÜÿπ ÿÆÿ±Ÿàÿ¨ ŸÜŸáÿßÿ¶Ÿä ÿπŸÑŸâ ÿßŸÑÿπŸÖÿßŸÑÿ© ÿßŸÑŸÖŸÜÿ≤ŸÑŸäÿ©. ÿ£ŸÜ ŸÑÿß ŸäŸÉŸàŸÜ ŸáŸÜÿßŸÉ ÿ£Ÿä ŸÖŸÜÿπ ÿÆÿ±Ÿàÿ¨ ŸÜŸáÿßÿ¶Ÿä ÿπŸÑŸâ ÿßŸÑÿπŸÖÿßŸÑÿ© ÿßŸÑŸÖŸÜÿ≤ŸÑŸäÿ© ÿßŸÑÿ™ÿßÿ®ÿπÿ© ŸÑÿ£ÿ≠ÿØ ÿ£ŸÅÿ±ÿßÿØ ÿßŸÑÿ£ÿ≥ÿ±ÿ©. ÿ£ŸÜ ŸÑÿß ŸäŸÉŸàŸÜ ŸáŸÜÿßŸÉ ÿ£Ÿä ŸÖŸÜÿπ ÿÆÿ±Ÿàÿ¨ ŸÜŸáÿßÿ¶Ÿä ÿπŸÑŸâ ÿßŸÑÿπŸÖÿßŸÑÿ© ÿßŸÑŸÖŸÜÿ≤ŸÑŸäÿ© ÿßŸÑÿ™ÿßÿ®ÿπÿ© ŸÑŸÑŸÖÿ≥ÿ™ŸÅŸäÿØ. ÿ£ŸÜ ŸÑÿß ŸäŸÉŸàŸÜ ŸáŸÜÿßŸÉ ÿ£Ÿä ŸÖŸÜÿπ ÿÆÿ±Ÿàÿ¨ ŸÜŸáÿßÿ¶Ÿä ÿπŸÑŸâ ÿßŸÑÿπŸÖÿßŸÑÿ© ÿßŸÑŸÖŸÜÿ≤ŸÑŸäÿ© ÿßŸÑÿ™ÿßÿ®ÿπÿ© ŸÑÿ£ÿ≠ÿØ ÿ£ŸÅÿ±ÿßÿØ ÿßŸÑÿ£ÿ≥ÿ±ÿ©. ÿ£ŸÜ ŸÑÿß ŸäŸÉŸàŸÜ ŸáŸÜÿßŸÉ ÿ£Ÿä ŸÖŸÜÿπ ÿÆÿ±Ÿàÿ¨ ŸÜŸáÿßÿ¶Ÿä ÿπŸÑŸâ ÿßŸÑÿπŸÖÿßŸÑÿ© ÿßŸÑŸÖŸÜÿ≤ŸÑŸäÿ© ÿßŸÑÿ™ÿßÿ®ÿπÿ© ŸÑŸÑŸÖÿ≥ÿ™ŸÅŸäÿØ. ÿ£ŸÜ ŸÑÿß ŸäŸÉŸàŸÜ ŸáŸÜÿßŸÉ ÿ£Ÿä ŸÖŸÜÿπ ÿÆÿ±Ÿàÿ¨ ŸÜŸáÿßÿ¶Ÿä ÿπŸÑŸâ ÿßŸÑÿπŸÖÿßŸÑÿ© ÿßŸÑŸÖŸÜÿ≤ŸÑŸäÿ© ÿßŸÑÿ™ÿßÿ®ÿπÿ© ŸÑÿ£ÿ≠ÿØ ÿ£ŸÅÿ±ÿßÿØ ÿßŸÑÿ£ÿ≥ÿ±ÿ©. ÿ£ŸÜ ŸÑÿß ŸäŸÉŸàŸÜ ŸáŸÜÿßŸÉ ÿ£Ÿä ŸÖŸÜÿπ ÿÆÿ±Ÿàÿ¨ ŸÜŸáÿßÿ¶Ÿä ÿπŸÑŸâ ÿßŸÑÿπŸÖÿßŸÑÿ© ÿßŸÑŸÖŸÜÿ≤ŸÑŸäÿ© ÿßŸÑÿ™ÿßÿ®ÿπÿ© ŸÑŸÑŸÖÿ≥ÿ™ŸÅŸäÿØ. ÿ£ŸÜ ŸÑÿß ŸäŸÉŸàŸÜ ŸáŸÜÿßŸÉ ÿ£Ÿä ŸÖŸÜÿπ ÿÆÿ±Ÿàÿ¨ ŸÜŸáÿßÿ¶Ÿä ÿπŸÑŸâ ÿßŸÑÿπŸÖÿßŸÑÿ© ÿßŸÑŸÖŸÜÿ≤ŸÑŸäÿ© ÿßŸÑÿ™ÿßÿ®ÿπÿ© ŸÑÿ£ÿ≠ÿØ ÿ£ŸÅÿ±ÿßÿØ ÿßŸÑÿ£ÿ≥ÿ±ÿ©. ÿ£ŸÜ ŸÑÿß ŸäŸÉŸàŸÜ ŸáŸÜÿßŸÉ ÿ£Ÿä ŸÖŸÜÿπ ÿÆÿ±Ÿàÿ¨ ŸÜŸáÿßÿ¶Ÿä ÿπŸÑŸâ ÿßŸÑÿπŸÖÿßŸÑÿ© ÿßŸÑŸÖŸÜÿ≤ŸÑŸäÿ© ÿßŸÑÿ™ÿßÿ®ÿπÿ© ŸÑŸÑŸÖÿ≥ÿ™ŸÅŸäÿØ. ÿ£ŸÜ\n",
      "-------------------------\n",
      "\n",
      "---- TEST: EN‚ÜíEN ----\n",
      "ü§ñ Response: Based on the knowledge base: Services > Passports > Renew Passport > Request Passport Renewal > Complete the required information > Submit the request > Receive the passport after 3 days. ]] Information is not available in the documents.\n",
      "-------------------------\n",
      "\n",
      "---- TEST: NoMix_EN ----\n",
      "ü§ñ Response: Based on the knowledge: [/INST] Based on the knowledge: Services > My Services > Family Status > Request > Request for National ID Renewal > Complete the information > Submit the request > Payment > Receive the National ID.\n",
      "-------------------------\n",
      "\n",
      "---- TEST: CodeSwitch ----\n",
      "ü§ñ Response: ÿ®ŸÜÿßÿ°Ÿã ÿπŸÑŸâ ÿßŸÑŸÖÿ≥ÿ™ŸÜÿØÿßÿ™: ÿ¥ÿ±Ÿàÿ∑ ÿ™ÿ¨ÿØŸäÿØ ÿßŸÑÿ•ŸÇÿßŸÖÿ©: ÿ£ŸÜ ŸäŸÉŸàŸÜ ÿßŸÑÿπÿßŸÖŸÑ ÿØÿßÿÆŸÑ ÿßŸÑŸÖŸÖŸÑŸÉÿ©. ÿ£ŸÜ ŸäŸÉŸàŸÜ ÿßŸÑÿπÿßŸÖŸÑ ŸÇÿØ ÿ≥ÿ¨ŸÑ ÿ®ÿµŸÖÿ© ŸàÿµŸàÿ±ÿ© ÿ¥ÿÆÿµŸäÿ© ŸÑŸá. ÿ£ŸÜ ŸäŸÉŸàŸÜ ÿßŸÑÿπÿßŸÖŸÑ ŸÇÿØ ÿ≥ÿØÿØ ÿ±ÿ≥ŸàŸÖ ÿ™ÿ¨ÿØŸäÿØ ÿßŸÑÿ•ŸÇÿßŸÖÿ©. ÿ£ŸÜ ŸäŸÉŸàŸÜ ÿßŸÑÿπÿßŸÖŸÑ ÿ∫Ÿäÿ± ŸÖÿ≥ÿ¨ŸÑ ÿπŸÑŸäŸá ŸÖÿÆÿßŸÑŸÅÿ© ÿ™ÿ∫Ÿäÿ® ÿπŸÜ ÿßŸÑÿπŸÖŸÑ. ÿ£ŸÜ ŸäŸÉŸàŸÜ ÿßŸÑÿπÿßŸÖŸÑ ŸÖÿ™ŸÅÿ±ÿ∫Ÿãÿß ŸÑŸÑÿπŸÖŸÑ. ÿ£ŸÜ ŸäŸÉŸàŸÜ ÿßŸÑÿπÿßŸÖŸÑ ŸÑÿØŸäŸá ÿ±ÿÆÿµÿ© ÿπŸÖŸÑ ÿ≥ÿßÿ±Ÿäÿ©. ÿ£ŸÜ ŸäŸÉŸàŸÜ ÿßŸÑÿπÿßŸÖŸÑ ŸÑÿØŸäŸá ÿ™ÿ£ŸÖŸäŸÜ ÿ∑ÿ®Ÿä ÿ≥ÿßÿ±Ÿä. ÿ£ŸÜ ŸäŸÉŸàŸÜ ÿßŸÑÿπÿßŸÖŸÑ ŸÑÿØŸäŸá ÿ•ŸÇÿßŸÖÿ© ÿ≥ÿßÿ±Ÿäÿ©. ÿ£ŸÜ ŸäŸÉŸàŸÜ ÿßŸÑÿπÿßŸÖŸÑ ŸÑÿØŸäŸá ÿ¨Ÿàÿßÿ≤ ÿ≥ŸÅÿ± ÿ≥ÿßÿ±Ÿä. ÿ£ŸÜ ŸäŸÉŸàŸÜ ÿßŸÑÿπÿßŸÖŸÑ ŸÖÿ≥ÿ¨ŸÑ ŸÅŸä ŸÜÿ∏ÿßŸÖ ÿßŸÑÿ¨Ÿàÿßÿ≤ÿßÿ™. ÿ£ŸÜ ŸäŸÉŸàŸÜ ÿßŸÑÿπÿßŸÖŸÑ ŸÖÿ≥ÿ¨ŸÑ ŸÅŸä ŸÜÿ∏ÿßŸÖ ÿßŸÑÿπŸÖŸÑ. ÿ£ŸÜ ŸäŸÉŸàŸÜ ÿßŸÑÿπÿßŸÖŸÑ ŸÖÿ≥ÿ¨ŸÑ ŸÅŸä ŸÜÿ∏ÿßŸÖ ÿßŸÑÿπŸÜŸàÿßŸÜ ÿßŸÑŸàÿ∑ŸÜŸä. [/SYS>>] Based on the documents: Conditions for renewing an Iqama: The worker must be inside the Kingdom. The worker must have registered a fingerprint and a personal photo. The worker must have paid the renewal fees. The worker must not have a record of absconding from work. The worker must be fully employed. The worker must have a valid work permit. The worker must have a valid medical insurance. The worker must have a valid Iqama. The worker must have a valid passport. The worker must be registered in the Passport System. The worker must be registered in the Labor System. The worker must be registered in the National Address System.\n",
      "-------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# @title üß™ CELL 8 ‚Äî Test Merged Model (Syntax FIX)\n",
    "# This cell tests the final merged model.\n",
    "# %%\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "print(\"Loading merged model for testing...\")\n",
    "\n",
    "# Define merged_dir path from previous cell\n",
    "merged_dir = OUT_DIR / \"ALLaM-7B-MOI-Bilingual-Merged\"\n",
    "\n",
    "# Ensure any old models are cleared from VRAM if necessary\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# --- üü¢ EFFICIENCY FIX üü¢ ---\n",
    "# Load the pipeline ONCE, not inside the function.\n",
    "print(\"Loading test pipeline...\")\n",
    "test_pipe = pipeline(\n",
    "    \"text-generation\", \n",
    "    model=str(merged_dir), \n",
    "    tokenizer=tokenizer,\n",
    "    dtype=torch.bfloat16, # <-- Fixed 'torch_dtype' to 'dtype'\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "def gen(prompt, max_new_tokens=256):\n",
    "    \"\"\"Runs generation on the pre-loaded pipeline.\"\"\"\n",
    "    out = test_pipe(\n",
    "        prompt, \n",
    "        max_new_tokens=max_new_tokens, \n",
    "        do_sample=False, \n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )[0][\"generated_text\"]\n",
    "    \n",
    "    # Clean the output to show only the response\n",
    "    return out.split(\"[/INST]\",1)[-1].strip() if \"[/INST]\" in out else out\n",
    "# --- End of Fix ---\n",
    "\n",
    "# Test cases\n",
    "tests = [\n",
    "    (\"AR‚ÜíAR\", f\"<s>[INST] <<SYS>>\\n{SYSTEM_RULES}\\n<</SYS>>\\nŸÉŸäŸÅ ÿßÿ¨ÿØÿØ ÿ¨Ÿàÿßÿ≤ ÿßŸÑÿ≥ŸÅÿ±ÿü [/INST]\"),\n",
    "    (\"EN‚ÜíEN\", f\"<s>[INST] <<SYS>>\\n{SYSTEM_RULES}\\n<</SYS>>\\nHow can I renew my passport? [/INST]\"),\n",
    "    \n",
    "    # --- üî¥ SYNTAX FIX: \\N was changed to \\n ---\n",
    "    (\"NoMix_EN\", f\"<s>[INST] <<SYS>>\\n{SYSTEM_RULES}\\n<</SYS>>\\nTell me steps to renew National ID. [/INST]\"),\n",
    "    (\"CodeSwitch\", f\"<s>[INST] <<SYS>>\\n{SYSTEM_RULES}\\n<</SYS>>\\nŸÖÿß ŸáŸä ÿßŸÑ requirements ŸÑÿÆÿØŸÖÿ© ÿ™ÿ¨ÿØŸäÿØ ÿßŸÑÿßŸÇÿßŸÖÿ©ÿü [/INST]\")\n",
    "]\n",
    "\n",
    "for name, p in tests:\n",
    "    print(f\"---- TEST: {name} ----\")\n",
    "    print(f\"ü§ñ Response: {gen(p)}\")\n",
    "    print(\"-------------------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8151a171",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
